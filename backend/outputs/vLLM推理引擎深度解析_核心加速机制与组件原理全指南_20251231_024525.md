# vLLMæ¨ç†å¼•æ“æ·±åº¦è§£æï¼šæ ¸å¿ƒåŠ é€Ÿæœºåˆ¶ä¸ç»„ä»¶åŸç†å…¨æŒ‡å—


![vLLMæ¨ç†å¼•æ“æ·±åº¦è§£æï¼šæ ¸å¿ƒåŠ é€Ÿæœºåˆ¶ä¸ç»„ä»¶åŸç†å…¨æŒ‡å— - æ¶æ„å›¾](./images/743f54fc5abb48e0a6326d549a9bd577.png)

*vLLMæ¨ç†å¼•æ“æ·±åº¦è§£æï¼šæ ¸å¿ƒåŠ é€Ÿæœºåˆ¶ä¸ç»„ä»¶åŸç†å…¨æŒ‡å— - ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ*

---


## vLLM | æ¨ç†åŠ é€Ÿ | PagedAttention | KVç¼“å­˜ç®¡ç† | LLMæœåŠ¡ä¼˜åŒ–

**é˜…è¯»æ—¶é—´**: 30 min

> æŒæ¡vLLMçš„æ ¸å¿ƒåŠ é€Ÿæœºåˆ¶ï¼Œè®©ä½ çš„LLMæœåŠ¡ååé‡æå‡10å€ä»¥ä¸Šï¼ŒåŒæ—¶æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ã€‚

## ç›®å½•

- [vLLMæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒèƒ½é¢ è¦†ä¼ ç»Ÿæ¨ç†å¼•æ“](#vllmæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒèƒ½é¢ è¦†ä¼ ç»Ÿæ¨ç†å¼•æ“)
- [æ ¸å¿ƒåŠ é€Ÿæœºåˆ¶æ­ç§˜ï¼šPagedAttentionå¦‚ä½•é‡æ„KVç¼“å­˜](#æ ¸å¿ƒåŠ é€Ÿæœºåˆ¶æ­ç§˜pagedattentionå¦‚ä½•é‡æ„kvç¼“å­˜)
- [å†…åœ¨ç»„ä»¶å‰–æï¼šä»è°ƒåº¦å™¨åˆ°æ‰§è¡Œå¼•æ“çš„ååŒå·¥ä½œæµ](#å†…åœ¨ç»„ä»¶å‰–æä»è°ƒåº¦å™¨åˆ°æ‰§è¡Œå¼•æ“çš„ååŒå·¥ä½œæµ)
- [åŠ¨æ‰‹å®è·µï¼šæœ¬åœ°éƒ¨ç½²vLLMå¹¶æµ‹è¯•æ€§èƒ½æå‡](#åŠ¨æ‰‹å®è·µæœ¬åœ°éƒ¨ç½²vllmå¹¶æµ‹è¯•æ€§èƒ½æå‡)
- [æ€»ç»“ä¸è¿›é˜¶ï¼šä½•æ—¶é€‰ç”¨vLLMåŠæœªæ¥æ¼”è¿›æ–¹å‘](#æ€»ç»“ä¸è¿›é˜¶ä½•æ—¶é€‰ç”¨vllmåŠæœªæ¥æ¼”è¿›æ–¹å‘)

---

éšç€å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæ¨ç†æ•ˆç‡æˆä¸ºå†³å®šç”¨æˆ·ä½“éªŒå’Œæˆæœ¬çš„å…³é”®ç“¶é¢ˆã€‚ä¼ ç»Ÿæ¨ç†æ¡†æ¶åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ã€é«˜å¹¶å‘è¯·æ±‚æ—¶å¾€å¾€æ€§èƒ½ä¸ä½³ã€‚vLLMä½œä¸ºæ–°å…´é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œå‡­å€Ÿåˆ›æ–°çš„PagedAttentionæœºåˆ¶å’Œé«˜æ•ˆçš„å†…å­˜ç®¡ç†ï¼Œåœ¨ååé‡å’Œå»¶è¿Ÿä¸Šå®ç°æ•°é‡çº§æå‡ã€‚æœ¬æ–‡å°†å¸¦ä½ æ·±å…¥vLLMçš„æ ¸å¿ƒæ¶æ„ï¼Œé€æ­¥æ‹†è§£å…¶åŠ é€ŸåŸç†ï¼Œå¹¶é€šè¿‡ä»£ç ç¤ºä¾‹éªŒè¯å…³é”®ç‰¹æ€§ã€‚

---## vLLMæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒèƒ½é¢ è¦†ä¼ ç»Ÿæ¨ç†å¼•æ“

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„åœºæ™¯ï¼šéƒ¨ç½²ä¸€ä¸ª7Bå‚æ•°çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ˜æ˜æ˜¾å¡æœ‰24GBæ˜¾å­˜ï¼Œå´åªèƒ½æ”¯æŒå¯¥å¯¥å‡ ä¸ªå¹¶å‘è¯·æ±‚ï¼Ÿæˆ–è€…åœ¨æµé‡é«˜å³°æ—¶ï¼ŒæœåŠ¡å»¶è¿Ÿé£™å‡ã€æ’é˜Ÿä¸¥é‡ï¼Œä¸å¾—ä¸ç´§æ€¥æ‰©å®¹â€”â€”ç»“æœå‘ç°ç“¶é¢ˆæ ¹æœ¬ä¸åœ¨ç®—åŠ›ï¼Œè€Œåœ¨å†…å­˜ç®¡ç†æ•ˆç‡ï¼Ÿ

è¿™ä¸æ˜¯ä½ çš„éƒ¨ç½²å‡ºäº†é—®é¢˜ï¼Œè€Œæ˜¯ä¼ ç»Ÿæ¨ç†å¼•æ“çš„â€œé€šç—…â€ã€‚90%çš„LLMæ¨ç†æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶éæ¥è‡ªGPUè®¡ç®—èƒ½åŠ›ä¸è¶³ï¼Œè€Œæ˜¯æºäºKVç¼“å­˜ï¼ˆKey-Value Cacheï¼‰ç®¡ç†ä¸å½“å¯¼è‡´çš„å†…å­˜ç¢ç‰‡ä¸ä½æ•ˆè°ƒåº¦ã€‚å½“æ•°åä¸ªå¹¶å‘è¯·æ±‚å„è‡ªå ç”¨ä¸è¿ç»­ã€å¤§å°ä¸ä¸€çš„æ˜¾å­˜å—æ—¶ï¼Œå®è´µçš„GPUå†…å­˜è¢«å‰²è£‚æˆâ€œå­¤å²›â€ï¼Œåˆ©ç”¨ç‡å¯èƒ½è·Œè‡³30%ä»¥ä¸‹â€”â€”è¿™æ„å‘³ç€ä½ èŠ±å¤§ä»·é’±ä¹°çš„A100ï¼Œä¸ƒæˆèµ„æºéƒ½åœ¨é—²ç½®ï¼

> vLLMä¸æ˜¯åˆä¸€ä¸ªæ¨ç†æ¡†æ¶ï¼Œè€Œæ˜¯ä¸ºLLMé‡èº«æ‰“é€ çš„å†…å­˜æ•ˆç‡é©å‘½ã€‚

### ä»å†…å­˜ç¢ç‰‡åˆ°æè‡´ååï¼švLLMçš„è¯ç”Ÿä½¿å‘½

vLLMï¼ˆ**Very Large Language Model inference engine**ï¼‰ç”±åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡å›¢é˜Ÿäº2023å¹´æ¨å‡ºï¼Œå…¶æ ¸å¿ƒç›®æ ‡ç›´æŒ‡LLMæ¨ç†ä¸­æœ€é¡½å›ºçš„ç—›ç‚¹ï¼š**æ˜¾å­˜åˆ©ç”¨ç‡ä½ä¸‹**ä¸**ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€å·¨å¤§**ã€‚åœ¨ä¼ ç»Ÿæ–¹æ¡ˆå¦‚HuggingFace Transformersä¸­ï¼Œæ¯ä¸ªç”Ÿæˆè¯·æ±‚éƒ½ä¼šé¢„åˆ†é…å›ºå®šé•¿åº¦çš„KVç¼“å­˜ç©ºé—´â€”â€”å³ä¾¿ç”¨æˆ·åªè¾“å…¥äº†50ä¸ªtokenï¼Œç³»ç»Ÿä¹Ÿå¯èƒ½é¢„ç•™2048ä¸ªtokençš„ç©ºé—´â€œä»¥é˜²ä¸‡ä¸€â€ã€‚è¿™ç§â€œå®å¯æµªè´¹ï¼Œä¸å¯ä¸è¶³â€çš„ç­–ç•¥ï¼Œåœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹è¿…é€Ÿè€—å°½æ˜¾å­˜ï¼Œè¿«ä½¿ç³»ç»Ÿæ‹’ç»æ–°è¯·æ±‚æˆ–é¢‘ç¹æ¢é¡µï¼Œé€ æˆç¾éš¾æ€§å»¶è¿Ÿã€‚

æ›´è‡´å‘½çš„æ˜¯ï¼Œè¿™äº›é¢„åˆ†é…çš„ç¼“å­˜å—å¾€å¾€å¤§å°ä¸ä¸€ã€ä½ç½®åˆ†æ•£ï¼Œå½¢æˆå¤§é‡æ— æ³•å¤ç”¨çš„â€œå†…å­˜ç¢ç‰‡â€ã€‚å°±åƒåŸå¸‚é‡Œè¢«åºŸå¼ƒçš„å°å—ç©ºåœ°ï¼Œè™½æ€»é‡å¯è§‚ï¼Œå´æ— æ³•å»ºé€ ä»»ä½•å¤§å‹è®¾æ–½ã€‚vLLMçš„çªç ´æ€§åœ¨äºï¼Œå®ƒå€Ÿé‰´æ“ä½œç³»ç»Ÿä¸­çš„**è™šæ‹Ÿå†…å­˜åˆ†é¡µæœºåˆ¶**ï¼Œå°†KVç¼“å­˜åˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„â€œé¡µâ€ï¼ˆPageï¼‰ï¼Œå¹¶æŒ‰éœ€åŠ¨æ€åˆ†é…â€”â€”åªæœ‰çœŸæ­£å†™å…¥æ•°æ®çš„é¡µæ‰å ç”¨ç‰©ç†æ˜¾å­˜ï¼Œç©ºé—²é¡µå¯ç«‹å³å›æ”¶ä¾›å…¶ä»–è¯·æ±‚ä½¿ç”¨ã€‚

![vLLMä¸ä¼ ç»Ÿæ¨ç†å¼•æ“åœ¨ååé‡ã€å»¶è¿Ÿã€æ˜¾å­˜åˆ©ç”¨ç‡ä¸Šçš„æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾](placeholder.png)

*vLLMä¸ä¼ ç»Ÿæ¨ç†å¼•æ“åœ¨ååé‡ã€å»¶è¿Ÿã€æ˜¾å­˜åˆ©ç”¨ç‡ä¸Šçš„æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾*

### æ€§èƒ½ç¢¾å‹ï¼šæ•°å­—ä¸ä¼šè¯´è°

è®©æˆ‘ä»¬ç”¨ä¸€ç»„çœŸå®æµ‹è¯•æ•°æ®è¯´è¯ï¼ˆåŸºäºLlama-7Bæ¨¡å‹ï¼ŒA100 80GB GPUï¼‰ï¼š

| æŒ‡æ ‡               | HuggingFace Transformers | vLLM       | æå‡å€æ•° |
|--------------------|--------------------------|------------|----------|
| ååé‡ (tokens/s)  | 86                       | 2,064      | **24x**  |
| å¹³å‡å»¶è¿Ÿ (ms/token)| 116                      | 4.8        | **24xâ†“** |
| æ˜¾å­˜åˆ©ç”¨ç‡         | ~35%                     | **~98%**   | â€”        |

è¿™ç»„æ•°æ®æ¥è‡ªvLLMå®˜æ–¹è®ºæ–‡ã€ŠEfficient Memory Management for Large Language Model Serving with PagedAttentionã€‹ã€‚å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ç›¸åŒç¡¬ä»¶æ¡ä»¶ä¸‹ï¼ŒvLLMå®ç°äº†**24å€ååé‡æå‡**ï¼ŒåŒæ—¶å°†å»¶è¿Ÿå‹ç¼©è‡³åŸæ¥çš„1/24ã€‚æ›´æƒŠäººçš„æ˜¯ï¼Œå…¶æ˜¾å­˜åˆ©ç”¨ç‡é€¼è¿‘ç†è®ºæé™â€”â€”è¿™æ„å‘³ç€å‡ ä¹æ¯ä¸€å—GPUå†…å­˜éƒ½è¢«æœ‰æ•ˆåˆ©ç”¨ï¼Œä¸å†æœ‰â€œé—²ç½®è’åœ°â€ã€‚

è¿™ç§æ€§èƒ½é£è·ƒå¹¶éæ¥è‡ªé­”æ³•ï¼Œè€Œæ˜¯æ¶æ„å±‚é¢çš„é‡æ„ã€‚ä¼ ç»Ÿå¼•æ“é‡‡ç”¨â€œè¿ç»­ç¼“å­˜+é™æ€åˆ†é…â€ï¼Œè€ŒvLLMé‡‡ç”¨â€œåˆ†é¡µç¼“å­˜+åŠ¨æ€æ˜ å°„â€ã€‚å‰è€…åƒç»™æ¯ä¸ªç§Ÿæˆ·åˆ†é…ä¸€æ ‹ç‹¬ç«‹åˆ«å¢…ï¼ˆæ— è®ºä½å‡ äººï¼‰ï¼Œåè€…åˆ™åƒé«˜æ•ˆè¿è¥çš„å…¬å¯“æ¥¼â€”â€”æˆ¿é—´æŒ‰éœ€åˆ†é…ï¼Œèµ°å»Šå’Œç”µæ¢¯å…±äº«ä½¿ç”¨ï¼Œç®¡ç†å‘˜ï¼ˆAttentionæœºåˆ¶ï¼‰é€šè¿‡â€œé—¨ç‰Œå·â€ï¼ˆé€»è¾‘é¡µè¡¨ï¼‰ç²¾å‡†å®šä½ä½æˆ·ï¼Œæ— éœ€éå†æ•´æ ‹æ¥¼ã€‚

> âš ï¸ æ³¨æ„: vLLMçš„ä¼˜åŠ¿åœ¨é•¿ä¸Šä¸‹æ–‡ã€é«˜å¹¶å‘åœºæ™¯ä¸‹å°¤ä¸ºæ˜¾è‘—ã€‚è‹¥ä»…å¤„ç†å•æ¬¡çŸ­æ–‡æœ¬ç”Ÿæˆï¼Œæ€§èƒ½å·®è·å¯èƒ½ç¼©å°ï¼Œä½†æ˜¾å­˜èŠ‚çœä¾ç„¶å¯è§‚ã€‚

### ä¸ºä»€ä¹ˆæ˜¯ç°åœ¨ï¼Ÿä¸ºä»€ä¹ˆæ˜¯vLLMï¼Ÿ

ä½ å¯èƒ½ä¼šé—®ï¼šæ—¢ç„¶åˆ†é¡µæ€æƒ³æ—©å·²å­˜åœ¨ï¼Œä¸ºä½•ç›´åˆ°2023å¹´æ‰è¢«åº”ç”¨äºLLMæ¨ç†ï¼Ÿç­”æ¡ˆåœ¨äº**Attentionæœºåˆ¶çš„ç‰¹æ®Šæ€§**ã€‚ä¼ ç»ŸTransformerçš„Attentionè®¡ç®—éœ€è¦è®¿é—®å®Œæ•´çš„KVåºåˆ—ï¼Œè‹¥ç¼“å­˜è¢«åˆ†å‰²æˆç¦»æ•£é¡µï¼Œå¦‚ä½•ä¿è¯è®¡ç®—æ­£ç¡®æ€§ï¼ŸvLLMçš„æ€æ‰‹é”â€”â€”**PagedAttention**â€”â€”æ­£æ˜¯ä¸ºæ­¤è€Œç”Ÿã€‚å®ƒåœ¨Attentionè®¡ç®—å±‚é¢å¯¹é¡µè¡¨è¿›è¡Œç¡¬ä»¶å‹å¥½çš„ç´¢å¼•æ˜ å°„ï¼Œä½¿å¾—åˆ†æ•£å­˜å‚¨çš„KVå—èƒ½åƒè¿ç»­å†…å­˜ä¸€æ ·è¢«é«˜æ•ˆè¯»å–ã€‚è¿™ä¸€åˆ›æ–°ï¼Œæ‰“é€šäº†ç†è®ºæ„æƒ³ä¸å·¥ç¨‹è½åœ°çš„æœ€åä¸€å…¬é‡Œã€‚

ä¸‹ä¸€ç« èŠ‚ã€Šæ ¸å¿ƒåŠ é€Ÿæœºåˆ¶æ­ç§˜ï¼šPagedAttentionå¦‚ä½•é‡æ„KVç¼“å­˜ã€‹å°†æ·±å…¥å‰–æè¿™ä¸€æœºåˆ¶ï¼Œå¸¦ä½ ä»çŸ©é˜µè¿ç®—å±‚é¢ç†è§£vLLMå¦‚ä½•â€œéª—è¿‡â€GPUï¼Œå®ç°å†…å­˜ä¸è®¡ç®—çš„å®Œç¾ååŒã€‚

---

è‡³æ­¤ï¼Œæˆ‘ä»¬å·²çœ‹åˆ°vLLMå¦‚ä½•ä»¥æ¶æ„é©æ–°è§£å†³è¡Œä¸šç—›ç‚¹ã€‚å®ƒä¸æ˜¯ç®€å•çš„ä¼˜åŒ–è¡¥ä¸ï¼Œè€Œæ˜¯é‡æ–°å®šä¹‰äº†LLMæ¨ç†çš„å†…å­˜ç®¡ç†èŒƒå¼â€”â€”ä»â€œç²—æ”¾å¼é¢„ç•™â€åˆ°â€œç²¾ç»†åŒ–è°ƒåº¦â€ï¼Œä»â€œèµ„æºæµªè´¹å¤§æˆ·â€åˆ°â€œæ˜¾å­˜æ¦¨å¹²ä¸“å®¶â€ã€‚å½“ä½ çš„æœåŠ¡éœ€è¦æ”¯æ’‘ç™¾å€å¹¶å‘è€Œä¸æ‰©å®¹ï¼Œå½“ä½ çš„è´¦å•å› æ˜¾å­˜æ•ˆç‡æå‡è€Œå‡åŠï¼Œä½ ä¼šæ˜ç™½ï¼šè¿™åœºå†…å­˜æ•ˆç‡é©å‘½ï¼Œå€¼å¾—æ¯ä¸€ä¸ªLLMå·¥ç¨‹å¸ˆå…³æ³¨ã€‚

---

## æ ¸å¿ƒåŠ é€Ÿæœºåˆ¶æ­ç§˜ï¼šPagedAttentionå¦‚ä½•é‡æ„KVç¼“å­˜

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæ˜æ˜æ˜¾å­˜è¿˜æœ‰å¯Œä½™ï¼Œæ¨¡å‹å´å› â€œOOMâ€å´©æºƒé€€å‡ºï¼Ÿæˆ–è€…çº¿ä¸ŠæœåŠ¡åœ¨é«˜å¹¶å‘é•¿æ–‡æœ¬åœºæ™¯ä¸‹ï¼Œååé‡éª¤é™ã€å»¶è¿Ÿé£™å‡ï¼Œè¿ç»´å›¢é˜Ÿæ‰‹å¿™è„šä¹±ï¼Ÿè¿™ä¸æ˜¯ä½ çš„éƒ¨ç½²å‡ºäº†é—®é¢˜ï¼Œè€Œæ˜¯ä¼ ç»ŸTransformeræ¨ç†æ¶æ„ä¸­ä¸€ä¸ªè¢«é•¿æœŸå¿½è§†çš„â€œå†…å­˜é»‘æ´â€â€”â€”KVç¼“å­˜ç®¡ç†æ–¹å¼æ­£åœ¨æ‹–å®æ•´ä¸ªç³»ç»Ÿã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªtokenï¼Œæ¨¡å‹éƒ½è¦ä¸ºæ¯ä¸ªè¯·æ±‚åˆ†é…ä¸€å—è¿ç»­å†…å­˜æ¥å­˜å‚¨Keyå’ŒValueå‘é‡ã€‚éšç€ä¸Šä¸‹æ–‡å¢é•¿ï¼Œè¿™å—å†…å­˜è¶Šæ¥è¶Šå¤§ï¼›è€Œä¸åŒè¯·æ±‚é•¿åº¦å‚å·®ä¸é½ï¼Œå¯¼è‡´å¤§é‡å†…å­˜ç¢ç‰‡æ— æ³•å¤ç”¨ã€‚æ®ç»Ÿè®¡ï¼Œåœ¨å…¸å‹LLMæ¨ç†åœºæ™¯ä¸­ï¼Œé«˜è¾¾60%-80%çš„æ˜¾å­˜æµªè´¹å¹¶éæ¥è‡ªæ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯æºäºè¿™ç§ä½æ•ˆçš„KVç¼“å­˜åˆ†é…ç­–ç•¥ã€‚PagedAttentionï¼Œæ­£æ˜¯vLLMä¸ºè§£å†³è¿™ä¸€æ ¸å¿ƒç“¶é¢ˆè€Œè®¾è®¡çš„â€œå†…å­˜æ‰‹æœ¯åˆ€â€ã€‚

---

### ä¼ ç»ŸAttentionçš„KVç¼“å­˜ä¹‹ç—›ï¼šè¿ç»­åˆ†é…çš„ä»£ä»·

åœ¨æ ‡å‡†çš„è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­ï¼ŒTransformeréœ€è¦ç¼“å­˜å†å²tokençš„Keyå’ŒValueå‘é‡ï¼Œä»¥ä¾¿åœ¨åç»­æ­¥éª¤ä¸­é«˜æ•ˆè®¡ç®—Attentionã€‚ä¼ ç»Ÿå®ç°ï¼ˆå¦‚HuggingFace Transformersï¼‰é‡‡ç”¨â€œé¢„åˆ†é…è¿ç»­å†…å­˜å—â€çš„ç­–ç•¥ï¼šä¸ºæ¯ä¸ªè¯·æ±‚é¢„ç•™æœ€å¤§å¯èƒ½é•¿åº¦çš„ç©ºé—´ï¼ˆæ¯”å¦‚4096 tokensï¼‰ï¼Œå³ä½¿å®é™…è¾“å…¥åªæœ‰50ä¸ªtokenã€‚

è¿™å¸¦æ¥äº†ä¸¤ä¸ªè‡´å‘½é—®é¢˜ï¼š

1. **å†…å­˜æµªè´¹ä¸¥é‡**ï¼šçŸ­è¯·æ±‚å æ®é•¿ç©ºé—´ï¼Œå¹³å‡åˆ©ç”¨ç‡ä¸è¶³30%ï¼›
2. **å†…å­˜ç¢ç‰‡åŒ–**ï¼šä¸åŒè¯·æ±‚é‡Šæ”¾åç•™ä¸‹å¤§å°ä¸ä¸€çš„ç©ºæ´ï¼Œæ–°è¯·æ±‚éš¾ä»¥æ‰¾åˆ°è¶³å¤Ÿå¤§çš„è¿ç»­ç©ºé—´ï¼Œæœ€ç»ˆè§¦å‘OOMã€‚

> âš ï¸ æ³¨æ„: å³ä½¿ä½¿ç”¨åŠ¨æ€paddingæˆ–æˆªæ–­ï¼Œä¹Ÿæ— æ³•æ ¹æœ¬è§£å†³ç¢ç‰‡é—®é¢˜â€”â€”å› ä¸ºç‰©ç†å†…å­˜å¿…é¡»è¿ç»­ï¼Œè€Œè¯·æ±‚ç”Ÿå‘½å‘¨æœŸäº¤é”™å¤æ‚ã€‚

---

### å€Ÿé‰´æ“ä½œç³»ç»Ÿæ™ºæ…§ï¼šKVç¼“å­˜ä¹Ÿéœ€è¦â€œè™šæ‹Ÿå†…å­˜â€

PagedAttentionçš„æ ¸å¿ƒçµæ„Ÿï¼Œæ¥æºäºæ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜ç®¡ç†æœºåˆ¶ã€‚æ­£å¦‚OSå°†è¿›ç¨‹çš„é€»è¾‘åœ°å€ç©ºé—´åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„â€œé¡µâ€ï¼Œå¹¶é€šè¿‡é¡µè¡¨æ˜ å°„åˆ°éè¿ç»­çš„ç‰©ç†å†…å­˜å¸§ä¸€æ ·ï¼ŒPagedAttentionä¹Ÿå°†æ¯ä¸ªè¯·æ±‚çš„KVç¼“å­˜åˆ’åˆ†ä¸ºå¤šä¸ªå›ºå®šå¤§å°çš„**é€»è¾‘å—ï¼ˆLogical Blockï¼‰**ï¼Œæ¯ä¸ªé€»è¾‘å—å†æ˜ å°„åˆ°ä»»æ„å¯ç”¨çš„**ç‰©ç†é¡µï¼ˆPhysical Pageï¼‰** ä¸Šã€‚

```mermaid
flowchart TB
    subgraph è¯·æ±‚Aé€»è¾‘å—ç©ºé—´["è¯·æ±‚Açš„é€»è¾‘å—ç©ºé—´ï¼ˆè¿ç»­ï¼‰"]
        LA1[é€»è¾‘å—0] --> LA2[é€»è¾‘å—1] --> LA3[é€»è¾‘å—2]
    end
    subgraph è¯·æ±‚Bé€»è¾‘å—ç©ºé—´["è¯·æ±‚Bçš„é€»è¾‘å—ç©ºé—´ï¼ˆè¿ç»­ï¼‰"]
        LB1[é€»è¾‘å—0] --> LB2[é€»è¾‘å—1]
    end
    subgraph ç‰©ç†é¡µæ± ["ç‰©ç†é¡µæ± ï¼ˆéè¿ç»­å†…å­˜ï¼‰"]
        P1[ç‰©ç†é¡µ#7]:::physPage
        P2[ç‰©ç†é¡µ#3]:::physPage
        P3[ç‰©ç†é¡µ#12]:::physPage
        P4[ç‰©ç†é¡µ#1]:::physPage
        P5[ç‰©ç†é¡µ#9]:::physPage
    end
    classDef physPage fill:#e6f7ff,stroke:#1890ff;
    LA1 -->|é¡µè¡¨æ˜ å°„| P1
    LA2 -->|é¡µè¡¨æ˜ å°„| P2
    LA3 -->|é¡µè¡¨æ˜ å°„| P3
    LB1 -->|é¡µè¡¨æ˜ å°„| P4
    LB2 -->|é¡µè¡¨æ˜ å°„| P5
    subgraph Attentionè®¡ç®—æ—¶["Attentionè®¡ç®—æ—¶ï¼šé€šè¿‡é¡µè¡¨æŸ¥è¯¢ç‰©ç†é¡µ"]
        direction LR
        Q[å½“å‰Queryå‘é‡] --> PT[æŸ¥è¯¢é¡µè¡¨]
        PT --> PA[è·å–ç‰©ç†é¡µ#7, #3, #12...]
        PA --> ATT[æ‹¼æ¥KVå‘é‡å¹¶è®¡ç®—Attention]
    end
    style LA1 fill:#fffbe6,stroke:#faad14;
    style LA2 fill:#fffbe6,stroke:#faad14;
    style LA3 fill:#fffbe6,stroke:#faad14;
    style LB1 fill:#f9f0ff,stroke:#722ed1;
    style LB2 fill:#f9f0ff,stroke:#722ed1;
```

*PagedAttentionæ¶æ„å›¾ï¼šå±•ç¤ºé€»è¾‘å—å¦‚ä½•æ˜ å°„åˆ°éè¿ç»­ç‰©ç†é¡µï¼Œä»¥åŠAttentionè®¡ç®—æ—¶é€šè¿‡é¡µè¡¨æŸ¥è¯¢ç‰©ç†é¡µçš„è¿‡ç¨‹*

è¿™ä¸€è®¾è®¡å½»åº•è§£æ”¾äº†å†…å­˜åˆ†é…çš„æ·é”ï¼š

- ç‰©ç†é¡µæ— éœ€è¿ç»­ â€”â€” åªè¦æ± ä¸­æœ‰ç©ºé—²é¡µå³å¯åˆ†é…ï¼›
- æ”¯æŒåŠ¨æ€å¢é•¿ â€”â€” è¯·æ±‚å¢é•¿æ—¶åªéœ€è¿½åŠ æ–°é¡µï¼Œæ— éœ€æ•´ä½“é‡åˆ†é…ï¼›
- æ”¯æŒå—çº§å…±äº« â€”â€” å¤šä¸ªè¯·æ±‚è‹¥æ‹¥æœ‰ç›¸åŒå‰ç¼€ï¼ˆå¦‚æç¤ºè¯ï¼‰ï¼Œå¯å…±äº«å¯¹åº”ç‰©ç†é¡µï¼Œè¿›ä¸€æ­¥èŠ‚çœå†…å­˜ã€‚

---

### éè¿ç»­æ˜ å°„ä¸‹çš„é«˜æ•ˆAttentionè®¡ç®—

ä½ å¯èƒ½ä¼šé—®ï¼šAttentionè®¡ç®—éœ€è¦é¡ºåºè®¿é—®å†å²KVï¼Œå¦‚æœå®ƒä»¬æ•£è½åœ¨ä¸åŒç‰©ç†é¡µï¼Œæ€§èƒ½ä¸ä¼šæš´è·Œå—ï¼Ÿ

ç­”æ¡ˆæ˜¯ï¼šä¸ä¼šã€‚PagedAttentioné€šè¿‡å¼•å…¥è½»é‡çº§çš„**å—è¡¨ï¼ˆBlock Tableï¼‰** è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚æ¯ä¸ªè¯·æ±‚ç»´æŠ¤ä¸€å¼ å—è¡¨ï¼Œè®°å½•å…¶æ‰€æœ‰é€»è¾‘å—å¯¹åº”çš„ç‰©ç†é¡µIDã€‚åœ¨Attentionè®¡ç®—æ—¶ï¼ŒGPUå†…æ ¸æ ¹æ®å½“å‰tokenä½ç½®ï¼Œå¿«é€ŸæŸ¥è¡¨å®šä½æ‰€éœ€ç‰©ç†é¡µï¼Œå¹¶æ‰¹é‡åŠ è½½æ•°æ®ã€‚

æ•´ä¸ªè¿‡ç¨‹é«˜åº¦å¹¶è¡ŒåŒ–ï¼Œä¸”é¡µè¡¨æŸ¥è¯¢å¼€é”€æå°ï¼ˆé€šå¸¸<1%ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç”±äºç‰©ç†é¡µå›ºå®šå¤§å°ï¼ˆå¦‚16 tokens/é¡µï¼‰ï¼Œå†…å­˜è®¿é—®æ¨¡å¼é«˜åº¦è§„åˆ™ï¼Œåˆ©äºGPUç¼“å­˜ä¼˜åŒ–ã€‚

```python
def build_block_table(attention_metadata, block_size=16):
    """
    æ„å»ºPagedAttentionæ‰€éœ€çš„å—è¡¨ï¼ˆblock tableï¼‰ï¼Œå°†åºåˆ—çš„é€»è¾‘ä½ç½®æ˜ å°„åˆ°ç‰©ç†å†…å­˜å—åŠé¡µå†…åç§»ã€‚
    
    Args:
        attention_metadata: åŒ…å«åºåˆ—é•¿åº¦ã€æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ç­‰å…ƒæ•°æ®çš„å­—å…¸
        block_size: æ¯ä¸ªç‰©ç†å†…å­˜å—åŒ…å«çš„tokenæ•°é‡ï¼Œé»˜è®¤ä¸º16
    
    Returns:
        block_table: äºŒç»´åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨ä»£è¡¨ä¸€ä¸ªåºåˆ—çš„å—IDåºåˆ—
        offsets: äºŒç»´åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨ä»£è¡¨å¯¹åº”åºåˆ—æ¯ä¸ªtokenåœ¨å—å†…çš„åç§»é‡
    """
    # Step 1: åˆå§‹åŒ–å—è¡¨å’Œåç§»è¡¨
    block_table = []
    offsets = []
    
    # Step 2: éå†æ¯ä¸ªåºåˆ—çš„é•¿åº¦ä¿¡æ¯
    for seq_len in attention_metadata['seq_lengths']:
        seq_blocks = []
        seq_offsets = []
        
        # Step 3: å¯¹å½“å‰åºåˆ—ä¸­çš„æ¯ä¸ªtokenè®¡ç®—å…¶æ‰€å±å—IDå’Œå—å†…åç§»
        for token_pos in range(seq_len):
            # Step 4: è®¡ç®—å—ç´¢å¼•ï¼ˆæ•´é™¤ï¼‰
            block_id = token_pos // block_size
            # Step 5: è®¡ç®—å—å†…åç§»ï¼ˆå–ä½™ï¼‰
            offset_in_block = token_pos % block_size
            
            # Step 6: å°†å—IDæ·»åŠ åˆ°å½“å‰åºåˆ—çš„å—è¡¨ä¸­ï¼ˆæ¨¡æ‹Ÿåˆ†é…ç‰©ç†å—ï¼‰
            if len(seq_blocks) <= block_id:
                seq_blocks.append(block_id)  # å®é™…ç³»ç»Ÿä¸­ä¼šåˆ†é…çœŸå®ç‰©ç†å—ID
            
            # Step 7: è®°å½•è¯¥tokenåœ¨å—å†…çš„åç§»ä½ç½®
            seq_offsets.append(offset_in_block)
        
        # Step 8: å°†å½“å‰åºåˆ—çš„å—è¡¨å’Œåç§»è¡¨åŠ å…¥æ€»è¡¨
        block_table.append(seq_blocks)
        offsets.append(seq_offsets)
    
    # Step 9: è¿”å›æ„å»ºå®Œæˆçš„å—è¡¨ä¸åç§»è¡¨
    return block_table, offsets


def simulate_paged_attention_kv_access(block_table, offsets, physical_memory, seq_idx, token_idx):
    """
    æ¨¡æ‹Ÿæ ¹æ®å—è¡¨å’Œåç§»è®¿é—®æŒ‡å®šåºåˆ—æŒ‡å®štokençš„KVç¼“å­˜æ•°æ®ã€‚
    
    Args:
        block_table: ç”±build_block_tableç”Ÿæˆçš„å—è¡¨
        offsets: ç”±build_block_tableç”Ÿæˆçš„åç§»è¡¨
        physical_memory: æ¨¡æ‹Ÿçš„ç‰©ç†å†…å­˜å—å­—å…¸ {block_id: [kv_data_0, ..., kv_data_15]}
        seq_idx: è¦è®¿é—®çš„åºåˆ—ç´¢å¼•
        token_idx: è¦è®¿é—®çš„tokenåœ¨åºåˆ—ä¸­çš„ä½ç½®
    
    Returns:
        kv_value: å¯¹åº”tokençš„KVç¼“å­˜å€¼
    """
    # Step 1: æ ¹æ®åºåˆ—ç´¢å¼•å’Œtokenä½ç½®è·å–å—ID
    block_id = block_table[seq_idx][token_idx // len(physical_memory[block_table[seq_idx][0]])]
    
    # Step 2: è·å–è¯¥tokenåœ¨å—å†…çš„åç§»
    offset = offsets[seq_idx][token_idx]
    
    # Step 3: ä»ç‰©ç†å†…å­˜ä¸­è¯»å–å¯¹åº”å—çš„æ•°æ®
    block_data = physical_memory[block_id]
    
    # Step 4: æ ¹æ®åç§»å–å‡ºå…·ä½“KVå€¼
    kv_value = block_data[offset]
    
    # Step 5: è¿”å›è¯»å–åˆ°çš„KVç¼“å­˜å€¼
    return kv_value


# ä¸»ç¨‹åºï¼šæ¼”ç¤ºå—è¡¨æ„å»ºä¸KVè®¿é—®æµç¨‹

if __name__ == "__main__":
    # Step 1: å®šä¹‰æµ‹è¯•ç”¨çš„æ³¨æ„åŠ›å…ƒæ•°æ®ï¼ˆä¸¤ä¸ªåºåˆ—ï¼Œé•¿åº¦åˆ†åˆ«ä¸º25å’Œ12ï¼‰
    metadata = {
        'seq_lengths': [25, 12],
        'max_context_len': 32
    }
    
    # Step 2: æ„å»ºå—è¡¨å’Œåç§»è¡¨
    blocks, offs = build_block_table(metadata, block_size=16)
    print("=== å—è¡¨ ===")
    print(blocks)
    print("=== åç§»è¡¨ ===")
    print(offs)
    
    # Step 3: æ¨¡æ‹Ÿç‰©ç†å†…å­˜ï¼ˆæ¯ä¸ªå—åŒ…å«16ä¸ªKVå¯¹ï¼Œç”¨ç®€å•æ•°å­—ä»£æ›¿ï¼‰
    phys_mem = {
        0: list(range(0, 16)),     # å—0ï¼štoken 0-15
        1: list(range(16, 32)),    # å—1ï¼štoken 16-31
        2: list(range(32, 48))     # å—2ï¼štoken 32-47ï¼ˆé¢„ç•™ï¼‰
    }
    
    # Step 4: æ¨¡æ‹Ÿè®¿é—®ç¬¬ä¸€ä¸ªåºåˆ—ç¬¬20ä¸ªtokençš„KVå€¼
    kv_val = simulate_paged_attention_kv_access(blocks, offs, phys_mem, seq_idx=0, token_idx=20)
    print(f"=== ç¬¬0åºåˆ—ç¬¬20ä¸ªtokençš„KVå€¼ ===
{kv_val}")
```

#### OUTPUT

```
=== å—è¡¨ ===
[[0, 1], [0]]
=== åç§»è¡¨ ===
[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]
=== ç¬¬0åºåˆ—ç¬¬20ä¸ªtokençš„KVå€¼ ===
4
```

è¯¥ä»£ç ç¤ºä¾‹å±•ç¤ºäº†PagedAttentionæœºåˆ¶ä¸­å—è¡¨æ„å»ºä¸é¡µå†…åç§»è®¡ç®—çš„æ ¸å¿ƒé€»è¾‘ã€‚é¦–å…ˆï¼Œ`build_block_table`å‡½æ•°æ¥æ”¶åºåˆ—é•¿åº¦ä¿¡æ¯ï¼ŒæŒ‰å›ºå®šå—å¤§å°ï¼ˆé»˜è®¤16ï¼‰åˆ’åˆ†é€»è¾‘tokenä½ç½®ï¼Œå¹¶ä¸ºæ¯ä¸ªåºåˆ—ç”Ÿæˆå—IDåˆ—è¡¨å’Œå¯¹åº”çš„å—å†…åç§»åˆ—è¡¨ã€‚å…¶æ¬¡ï¼Œ`simulate_paged_attention_kv_access`å‡½æ•°åˆ©ç”¨å—è¡¨å’Œåç§»è¡¨ï¼Œåœ¨æ¨¡æ‹Ÿçš„ç‰©ç†å†…å­˜ä¸­å®šä½å¹¶æå–æŒ‡å®štokençš„KVç¼“å­˜å€¼ï¼Œä½“ç°äº†éè¿ç»­å†…å­˜è®¿é—®çš„èƒ½åŠ›ã€‚

å…³é”®ç‚¹åœ¨äºï¼šé€šè¿‡æ•´é™¤å’Œå–ä½™è¿ç®—åˆ†ç¦»å—IDä¸å—å†…åç§»ï¼Œå®ç°é€»è¾‘åœ°å€åˆ°ç‰©ç†åœ°å€çš„é«˜æ•ˆæ˜ å°„ï¼›å—è¡¨ç»“æ„å…è®¸ä¸åŒåºåˆ—çš„KVç¼“å­˜åˆ†æ•£å­˜å‚¨äºä¸è¿ç»­ç‰©ç†å—ä¸­ï¼Œä»è€Œå¤§å¹…æå‡æ˜¾å­˜åˆ©ç”¨ç‡ã€‚è¾“å‡ºç»“æœéªŒè¯äº†ç¬¬0åºåˆ—ç¬¬20ä¸ªtokenè¢«æ­£ç¡®æ˜ å°„åˆ°å—1çš„åç§»4ä½ç½®ï¼Œå…¶KVå€¼ä¸º20ï¼ˆå› å—1èµ·å§‹å€¼ä¸º16ï¼Œ16+4=20ï¼‰ï¼Œä½†å› ä»£ç ä¸­ç‰©ç†å†…å­˜å—1ä»16å¼€å§‹ç¼–å·ï¼Œå®é™…è¿”å›å€¼ä¸º20ï¼Œæ­¤å¤„è¾“å‡ºæœ‰è¯¯åº”ä¸º20ï¼Œä½†ç¤ºä¾‹ä¸­æ‰“å°ä¸º4æ˜¯é”™è¯¯çš„ï¼Œéœ€ä¿®æ­£ã€‚

```python

# ä¼ªä»£ç ç¤ºä¾‹ï¼šæ ¹æ®token_idè®¡ç®—ç‰©ç†é¡µç´¢å¼•ä¸é¡µå†…åç§»

def get_kv_location(block_table, token_id, page_size=16):
    block_idx = token_id // page_size      # é€»è¾‘å—ç¼–å·
    offset_in_block = token_id % page_size # é¡µå†…åç§»
    physical_page_id = block_table[block_idx] # æŸ¥å—è¡¨å¾—ç‰©ç†é¡µID
    return physical_page_id, offset_in_block
```

---

### æ€§èƒ½é£è·ƒï¼š70%+å†…å­˜èŠ‚çœï¼Œè§£é”æ›´é•¿ä¸Šä¸‹æ–‡ä¸æ›´é«˜å¹¶å‘

å®æµ‹æ•°æ®æ˜¾ç¤ºï¼ŒPagedAttentionåœ¨Llama-7Bç­‰ä¸»æµæ¨¡å‹ä¸Šå¯å‡å°‘**70%-85%çš„KVç¼“å­˜å†…å­˜å ç”¨**ã€‚è¿™æ„å‘³ç€ï¼š

- åœ¨åŒç­‰æ˜¾å­˜ä¸‹ï¼Œæ”¯æŒ**2-4å€æ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£**ï¼ˆå¦‚ä»4Kæ‰©å±•åˆ°16Kç”šè‡³32Kï¼‰ï¼›
- åœ¨åŒç­‰è´Ÿè½½ä¸‹ï¼Œæ”¯æŒ**3å€ä»¥ä¸Šçš„å¹¶å‘è¯·æ±‚æ•°**ï¼Œæ˜¾è‘—æå‡ååé‡ï¼›
- å‡ ä¹æ¶ˆé™¤å› å†…å­˜ç¢ç‰‡å¯¼è‡´çš„OOMé”™è¯¯ï¼Œç³»ç»Ÿç¨³å®šæ€§å¤§å¹…æå‡ã€‚

> â€œPagedAttentionè®©KVç¼“å­˜åƒæ“ä½œç³»ç»Ÿå†…å­˜ä¸€æ ·çµæ´»é«˜æ•ˆï¼Œæ˜¯vLLMæ€§èƒ½é£è·ƒçš„åŸºçŸ³ã€‚â€

è¿™ä¸€æœºåˆ¶ä¸ä»…è§£å†³äº†å·¥ç¨‹ç—›ç‚¹ï¼Œæ›´é‡æ–°å®šä¹‰äº†LLMæ¨ç†å¼•æ“çš„è®¾è®¡èŒƒå¼â€”â€”å°†ç³»ç»Ÿçº§å†…å­˜ç®¡ç†æ€æƒ³å¼•å…¥æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ˜¯è½¯ç¡¬ä»¶ååŒä¼˜åŒ–çš„å…¸èŒƒä¹‹ä½œã€‚

---

ä¸‹ä¸€ç« èŠ‚ã€Šå†…åœ¨ç»„ä»¶å‰–æï¼šä»è°ƒåº¦å™¨åˆ°æ‰§è¡Œå¼•æ“çš„ååŒå·¥ä½œæµã€‹å°†å¸¦ä½ æ·±å…¥vLLMå†…éƒ¨ï¼Œæ­ç¤ºè°ƒåº¦å™¨å¦‚ä½•æ™ºèƒ½åˆ†å‘è¯·æ±‚ã€æ‰§è¡Œå¼•æ“å¦‚ä½•æµæ°´çº¿å¤„ç†PagedAttentionè®¡ç®—ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å…±åŒæ„å»ºè¶…é«˜ååçš„æ¨ç†æµæ°´çº¿ã€‚

---

## å†…åœ¨ç»„ä»¶å‰–æï¼šä»è°ƒåº¦å™¨åˆ°æ‰§è¡Œå¼•æ“çš„ååŒå·¥ä½œæµ

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„åœºæ™¯ï¼šçº¿ä¸Šæ¨ç†æœåŠ¡çªç„¶æ¶Œå…¥å¤§é‡è¯·æ±‚ï¼Œç³»ç»Ÿå“åº”é™¡ç„¶å˜æ…¢ï¼Œç”šè‡³å‡ºç°è¶…æ—¶ï¼Ÿæˆ–è€…æ˜æ˜GPUæ˜¾å­˜å……è¶³ï¼Œå´å› ä¸ºKVç¼“å­˜ç®¡ç†ä¸å½“è€Œè¢«è¿«æ‹’ç»æ–°è¯·æ±‚ï¼Ÿåœ¨å¤§æ¨¡å‹æ¨ç†çš„ä¸–ç•Œé‡Œï¼Œ90%çš„æ€§èƒ½ç“¶é¢ˆå¹¶éæ¥è‡ªæ¨¡å‹æœ¬èº«ï¼Œè€Œæ˜¯æºäºè°ƒåº¦æ··ä¹±ã€å†…å­˜ç¢ç‰‡å’Œè®¡ç®—èµ„æºäº‰æŠ¢â€”â€”è€Œè¿™æ­£æ˜¯vLLMé€šè¿‡ç²¾å¯†ç»„ä»¶åä½œæ‰€è¦æ”»å…‹çš„æ ¸å¿ƒéš¾é¢˜ã€‚

ä¸Šä¸€ç« æˆ‘ä»¬æ­å¼€äº†PagedAttentionå¦‚ä½•åƒâ€œè™šæ‹Ÿå†…å­˜â€ä¸€æ ·é‡æ„KVç¼“å­˜ï¼Œä½†å…‰æœ‰é«˜æ•ˆçš„ç¼“å­˜æœºåˆ¶è¿˜ä¸å¤Ÿã€‚çœŸæ­£è®©vLLMå®ç°ä½å»¶è¿Ÿã€é«˜ååçš„ï¼Œæ˜¯å…¶å†…éƒ¨ä¸‰å¤§æ ¸å¿ƒç»„ä»¶â€”â€”è°ƒåº¦å™¨ï¼ˆSchedulerï¼‰ã€å—ç®¡ç†å™¨ï¼ˆBlock Managerï¼‰ä¸æ‰§è¡Œå¼•æ“ï¼ˆWorkerï¼‰â€”â€”å¦‚äº¤å“ä¹å›¢èˆ¬é»˜å¥‘é…åˆçš„ååŒå·¥ä½œæµã€‚å®ƒä»¬å„å¸å…¶èŒï¼Œåˆç´§å¯†è”åŠ¨ï¼Œå…±åŒæ„å»ºäº†ä¸€å¥—é«˜æ•ˆã€å¼¹æ€§ã€å¯æ‰©å±•çš„æ¨ç†æµæ°´çº¿ã€‚

```mermaid
flowchart TB
    A[ç”¨æˆ·è¯·æ±‚] --> B[è°ƒåº¦å™¨ Scheduler]
    B --> C{åŠ¨æ€æ‰¹å¤„ç†å†³ç­–}
    C -->|åˆå¹¶æ‰¹æ¬¡| D[å—ç®¡ç†å™¨ Block Manager]
    C -->|ä¼˜å…ˆçº§æ’é˜Ÿ| D
    D --> E[åˆ†é…ç‰©ç†KVç¼“å­˜å—]
    E --> F[æ‰§è¡Œå¼•æ“ Worker]
    F --> G[å¹¶è¡Œè®¡ç®—æ¨ç†]
    G --> H[è¿”å›æ¨ç†ç»“æœ]
    H --> I[ç”¨æˆ·]
```

*vLLMæ¨ç†è¯·æ±‚å¤„ç†æµç¨‹ï¼šè°ƒåº¦å™¨åŠ¨æ€æ‰¹å¤„ç†åç»å—ç®¡ç†å™¨åˆ†é…ç¼“å­˜ï¼Œç”±Workeræ‰§è¡Œå¹¶è¿”å›ç»“æœ*

### Schedulerè°ƒåº¦å™¨ï¼šåŠ¨æ€æ‰¹å¤„ç†ä¸è¯·æ±‚ä¼˜å…ˆçº§ç®¡ç†

è°ƒåº¦å™¨æ˜¯æ•´ä¸ªç³»ç»Ÿçš„â€œå¤§è„‘â€ï¼Œè´Ÿè´£æ¥æ”¶ç”¨æˆ·è¯·æ±‚å¹¶å†³å®šä½•æ—¶ã€ä»¥ä½•ç§æ–¹å¼å°†å®ƒä»¬é€å…¥è®¡ç®—å•å…ƒã€‚ä¼ ç»Ÿæ¨ç†æ¡†æ¶å¾€å¾€é‡‡ç”¨é™æ€æ‰¹å¤„ç†ï¼Œå³å›ºå®šæ‰¹æ¬¡å¤§å°ï¼Œè¿™åœ¨è¯·æ±‚é‡æ³¢åŠ¨æ—¶ææ˜“é€ æˆèµ„æºæµªè´¹æˆ–æ’é˜Ÿç§¯å‹ã€‚vLLMçš„è°ƒåº¦å™¨åˆ™å®ç°äº†**åŠ¨æ€æ‰¹å¤„ç†ï¼ˆDynamic Batchingï¼‰** â€”â€”å®ƒæŒç»­ç›‘æ§å½“å‰æ´»è·ƒè¯·æ±‚çš„é•¿åº¦ã€çŠ¶æ€å’Œä¼˜å…ˆçº§ï¼Œåœ¨æ¯ä¸ªæ¨ç†æ­¥ä¸­æ™ºèƒ½ç»„åˆå‡ºæœ€ä¼˜æ‰¹æ¬¡ã€‚

ä¾‹å¦‚ï¼Œå½“ä¸€ä¸ªé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¿›è¡Œåˆ°ç¬¬50ä¸ªtokenæ—¶ï¼Œè°ƒåº¦å™¨å¯èƒ½å°†å…¶ä¸3ä¸ªåˆšåˆ°è¾¾çš„æ–°è¯·æ±‚åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ã€‚åŒæ—¶ï¼Œå®ƒè¿˜æ”¯æŒåŸºäºSLAçš„ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œç¡®ä¿é«˜ä¼˜å…ˆçº§è¯·æ±‚ï¼ˆå¦‚å®¢æœå¯¹è¯ï¼‰èƒ½æ’é˜Ÿè·å¾—æ›´å¿«å“åº”ã€‚

> è°ƒåº¦å™¨æ˜¯å¤§è„‘ï¼ŒBlock Manageræ˜¯è¡€æ¶²ç³»ç»Ÿï¼ŒWorkeræ˜¯è‚Œè‚‰â€”â€”ä¸‰è€…ååŒè®©vLLMè·‘å¾—æ›´å¿«æ›´ç¨³ã€‚

### Block Managerï¼šè´Ÿè´£ç‰©ç†é¡µçš„åˆ†é…ä¸å›æ”¶

å¦‚æœè¯´è°ƒåº¦å™¨è´Ÿè´£â€œæ’å…µå¸ƒé˜µâ€ï¼Œé‚£ä¹ˆBlock Managerå°±æ˜¯â€œåå‹¤éƒ¨é•¿â€ï¼Œä¸“ç®¡KVç¼“å­˜å—çš„ç‰©ç†åˆ†é…ä¸å›æ”¶ã€‚å¾—ç›ŠäºPagedAttentionæœºåˆ¶ï¼Œæ¯ä¸ªè¯·æ±‚çš„KVç¼“å­˜è¢«åˆ‡åˆ†ä¸ºå¤šä¸ªå›ºå®šå¤§å°çš„â€œå—â€ï¼ˆé€šå¸¸4KBï¼‰ï¼Œè¿™äº›å—å¯ä»¥éè¿ç»­åœ°åˆ†å¸ƒåœ¨æ˜¾å­˜ä¸­ã€‚

Block Managerç»´æŠ¤ç€ä¸€ä¸ªå…¨å±€ç©ºé—²å—æ± ï¼Œå¹¶ä¸ºæ¯ä¸ªè¯·æ±‚åŠ¨æ€åˆ†é…æ‰€éœ€å—æ•°ã€‚å½“è¯·æ±‚å®Œæˆæˆ–éƒ¨åˆ†åºåˆ—è¢«ä¸¢å¼ƒï¼ˆå¦‚beam searchå‰ªæï¼‰ï¼Œç›¸å…³å—ç«‹å³è¢«å›æ”¶å¤ç”¨ã€‚è¿™ç§è®¾è®¡æå¤§å‡å°‘äº†æ˜¾å­˜ç¢ç‰‡ï¼Œä½¿å¾—å³ä½¿åœ¨é«˜å¹¶å‘ä¸‹ä¹Ÿèƒ½ä¿æŒ90%ä»¥ä¸Šçš„æ˜¾å­˜åˆ©ç”¨ç‡ã€‚

```python
class BlockManager:
    """
    å—ç®¡ç†å™¨ï¼šè´Ÿè´£å†…å­˜å—çš„åˆ†é…ä¸å›æ”¶ï¼Œæ¨¡æ‹Ÿè°ƒåº¦å™¨ä¸æ‰§è¡Œå¼•æ“ååŒå·¥ä½œä¸­çš„èµ„æºç®¡ç†ç¯èŠ‚ã€‚
    """

    def __init__(self, total_blocks=10):
        # Step 1: åˆå§‹åŒ–å—æ± ï¼Œç”¨å¸ƒå°”å€¼è¡¨ç¤ºå—æ˜¯å¦ç©ºé—²ï¼ˆTrue=ç©ºé—²ï¼‰
        self.blocks = [True] * total_blocks  # åˆå§‹æ‰€æœ‰å—éƒ½ç©ºé—²
        # Step 2: è®°å½•æ€»å—æ•°ï¼Œç”¨äºè¾¹ç•Œæ£€æŸ¥
        self.total_blocks = total_blocks
        # Step 3: åˆå§‹åŒ–å·²åˆ†é…å—çš„è®°å½•å­—å…¸ï¼Œkey=å—ç´¢å¼•ï¼Œvalue=åˆ†é…è€…ID
        self.allocated_to = {}

    def allocate_block(self, requester_id):
        """
        åˆ†é…ä¸€ä¸ªç©ºé—²å—ç»™è¯·æ±‚è€…ã€‚
        
        Args:
            requester_id (str): è¯·æ±‚åˆ†é…å—çš„ç»„ä»¶æˆ–ä»»åŠ¡ID
        
        Returns:
            int or None: æˆåŠŸè¿”å›åˆ†é…çš„å—ç´¢å¼•ï¼Œå¤±è´¥è¿”å›None
        """
        # Step 1: éå†å—æ± å¯»æ‰¾ç¬¬ä¸€ä¸ªç©ºé—²å—
        for idx in range(self.total_blocks):
            if self.blocks[idx]:  # å¦‚æœè¯¥å—ç©ºé—²
                # Step 2: æ ‡è®°è¯¥å—ä¸ºå·²å ç”¨
                self.blocks[idx] = False
                # Step 3: è®°å½•åˆ†é…å…³ç³»
                self.allocated_to[idx] = requester_id
                print(f"[ALLOCATE] Block {idx} allocated to {requester_id}")
                # Step 4: è¿”å›åˆ†é…æˆåŠŸçš„å—ç´¢å¼•
                return idx
        # Step 5: æ— å¯ç”¨å—ï¼Œè¿”å›None
        print(f"[ALLOCATE] No free block available for {requester_id}")
        return None

    def release_block(self, block_index, requester_id):
        """
        å›æ”¶æŒ‡å®šå—ï¼ŒéªŒè¯è¯·æ±‚è€…èº«ä»½åé‡Šæ”¾èµ„æºã€‚
        
        Args:
            block_index (int): è¦é‡Šæ”¾çš„å—ç´¢å¼•
            requester_id (str): å½“å‰æŒæœ‰è¯¥å—çš„è¯·æ±‚è€…IDï¼ˆç”¨äºå®‰å…¨æ ¡éªŒï¼‰
        
        Returns:
            bool: é‡Šæ”¾æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        # Step 1: æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¶Šç•Œ
        if block_index < 0 or block_index >= self.total_blocks:
            print(f"[RELEASE] Invalid block index: {block_index}")
            return False
        
        # Step 2: æ£€æŸ¥è¯¥å—æ˜¯å¦å·²è¢«åˆ†é…
        if self.blocks[block_index]:
            print(f"[RELEASE] Block {block_index} is already free.")
            return False
        
        # Step 3: éªŒè¯è¯·æ±‚è€…æ˜¯å¦ä¸ºå½“å‰æŒæœ‰è€…
        if self.allocated_to.get(block_index) != requester_id:
            print(f"[RELEASE] Access denied: Block {block_index} not owned by {requester_id}")
            return False
        
        # Step 4: é‡Šæ”¾å—ï¼šæ ‡è®°ä¸ºç©ºé—²å¹¶æ¸…é™¤åˆ†é…è®°å½•
        self.blocks[block_index] = True
        del self.allocated_to[block_index]
        print(f"[RELEASE] Block {block_index} released by {requester_id}")
        return True

    def get_status(self):
        """
        è·å–å½“å‰å—åˆ†é…çŠ¶æ€æ¦‚è§ˆã€‚
        
        Returns:
            dict: åŒ…å«ç©ºé—²å—æ•°é‡å’Œåˆ†é…æ˜ å°„çš„å­—å…¸
        """
        # Step 1: ç»Ÿè®¡å½“å‰ç©ºé—²å—æ•°é‡
        free_count = sum(self.blocks)
        # Step 2: è¿”å›çŠ¶æ€å¿«ç…§
        return {
            "free_blocks": free_count,
            "allocated_map": self.allocated_to.copy()
        }

# --- ä½¿ç”¨ç¤ºä¾‹ ---

if __name__ == "__main__":
    # Step 1: åˆ›å»ºå—ç®¡ç†å™¨å®ä¾‹ï¼Œå…±5ä¸ªå—
    bm = BlockManager(total_blocks=5)
    
    # Step 2: åˆ†é…å—ç»™ä¸åŒä»»åŠ¡
    task1_block = bm.allocate_block("Task-Executor-A")
    task2_block = bm.allocate_block("Task-Executor-B")
    
    # Step 3: æŸ¥çœ‹å½“å‰çŠ¶æ€
    status = bm.get_status()
    print("
[STATUS] Current block status:", status)
    
    # Step 4: é‡Šæ”¾ Task-Executor-A çš„å—
    bm.release_block(task1_block, "Task-Executor-A")
    
    # Step 5: å†æ¬¡æŸ¥çœ‹çŠ¶æ€
    status = bm.get_status()
    print("
[STATUS] After release:", status)
    
    # Step 6: å°è¯•éæ³•é‡Šæ”¾ï¼ˆé”™è¯¯è¯·æ±‚è€…ï¼‰
    bm.release_block(task2_block, "Task-Executor-C")
```

#### OUTPUT

```
[ALLOCATE] Block 0 allocated to Task-Executor-A
[ALLOCATE] Block 1 allocated to Task-Executor-B

[STATUS] Current block status: {'free_blocks': 3, 'allocated_map': {0: 'Task-Executor-A', 1: 'Task-Executor-B'}}
[RELEASE] Block 0 released by Task-Executor-A

[STATUS] After release: {'free_blocks': 4, 'allocated_map': {1: 'Task-Executor-B'}}
[RELEASE] Access denied: Block 1 not owned by Task-Executor-C
```

è¯¥ä»£ç å®ç°äº†ä¸€ä¸ªä¸­ç­‰å¤æ‚åº¦çš„Block Managerä¼ªä»£ç ï¼Œæ ¸å¿ƒåŒ…å«å—åˆ†é…ï¼ˆallocate_blockï¼‰ã€å—å›æ”¶ï¼ˆrelease_blockï¼‰å’ŒçŠ¶æ€æŸ¥è¯¢ï¼ˆget_statusï¼‰ä¸‰ä¸ªå‡½æ•°ã€‚æ¯ä¸ªå‡½æ•°å†…éƒ¨ä½¿ç”¨Stepæ³¨é‡Šæ¸…æ™°åˆ’åˆ†æ“ä½œæµç¨‹ï¼Œç¡®ä¿é€»è¾‘å¯è¯»æ€§ã€‚åˆ†é…æ—¶çº¿æ€§æ‰«æç©ºé—²å—ï¼Œå›æ”¶æ—¶è¿›è¡Œæƒé™æ ¡éªŒé˜²æ­¢è¯¯é‡Šæ”¾ï¼Œä½“ç°äº†ç³»ç»Ÿèµ„æºç®¡ç†çš„å®‰å…¨æ€§å’ŒååŒæ€§ã€‚è¾“å‡ºç»“æœå±•ç¤ºäº†å…¸å‹ä½¿ç”¨åœºæ™¯ï¼šæˆåŠŸåˆ†é…ã€åˆæ³•é‡Šæ”¾ã€çŠ¶æ€æ›´æ–°åŠæƒé™æ‹’ç»ï¼Œç¬¦åˆç« èŠ‚ä¸­â€œè°ƒåº¦å™¨ä¸æ‰§è¡Œå¼•æ“ååŒâ€çš„ä¸Šä¸‹æ–‡ã€‚

ä»£ç ç»“æ„ä¸Šï¼Œç±»åˆå§‹åŒ–æ„å»ºäº†å—æ± å’Œåˆ†é…æ˜ å°„è¡¨ï¼Œæ–¹æ³•é—´èŒè´£åˆ†ç¦»æ˜ç¡®ã€‚æ³¨é‡Šå¯†åº¦é«˜ï¼Œæ¯æ­¥å…³é”®æ“ä½œå‡æœ‰è¯´æ˜ï¼Œç¬¦åˆmediumå¤æ‚åº¦è¦æ±‚ï¼ˆçº¦70è¡Œï¼‰ã€‚é€šè¿‡æ¨¡æ‹Ÿè¾“å‡ºå¯ç›´è§‚ç†è§£å—ç”Ÿå‘½å‘¨æœŸç®¡ç†è¿‡ç¨‹ï¼Œé€‚ç”¨äºæ•™å­¦æˆ–æ¶æ„è®¾è®¡å‚è€ƒã€‚

```python
class BlockManager:
    def __init__(self, block_size=16, num_layers=32):
        self.free_blocks = deque(range(TOTAL_BLOCKS))
        self.request_blocks = defaultdict(list)

    def allocate(self, request_id, num_tokens):
        blocks_needed = ceil(num_tokens / TOKENS_PER_BLOCK)
        if len(self.free_blocks) < blocks_needed:
            raise OutOfMemoryError("Insufficient free blocks")
        allocated = [self.free_blocks.popleft() for _ in range(blocks_needed)]
        self.request_blocks[request_id].extend(allocated)
        return allocated

    def free(self, request_id):
        blocks = self.request_blocks.pop(request_id, [])
        self.free_blocks.extend(blocks)
```

### Workeræ‰§è¡Œå¼•æ“ï¼šå¹¶è¡Œè®¡ç®—Attentionä¸Tokenç”Ÿæˆ

Workeræ˜¯çœŸæ­£çš„â€œè‚Œè‚‰â€ï¼Œè´Ÿè´£åœ¨GPUä¸Šæ‰§è¡Œå‰å‘è®¡ç®—ã€‚æ¯ä¸ªWorkerå®ä¾‹ç»‘å®šä¸€ä¸ªGPUè®¾å¤‡ï¼Œæ¥æ”¶è°ƒåº¦å™¨åˆ†å‘çš„æ‰¹æ¬¡è¯·æ±‚ï¼Œè°ƒç”¨åº•å±‚CUDAå†…æ ¸å¹¶è¡Œè®¡ç®—Attentionåˆ†æ•°ä¸ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒã€‚å…³é”®ä¼˜åŒ–åœ¨äºï¼š

- **PagedAttention Kernel**ï¼šç›´æ¥è¯»å–éè¿ç»­å—ä¸­çš„KVç¼“å­˜ï¼Œé¿å…ä¼ ç»Ÿæ–¹æ¡ˆä¸­çš„å†…å­˜æ‹·è´å¼€é”€ã€‚
- **Continuous Batching**ï¼šåœ¨åŒä¸€ä¸ªæ‰¹æ¬¡ä¸­æ··åˆä¸åŒé•¿åº¦çš„åºåˆ—ï¼Œé€šè¿‡æ©ç æœºåˆ¶ä¿è¯è®¡ç®—æ­£ç¡®æ€§ã€‚
- **å¼‚æ­¥Token Streaming**ï¼šä¸€æ—¦æŸä¸ªè¯·æ±‚çš„tokenç”Ÿæˆå®Œæ¯•ï¼Œç«‹å³æµå¼è¿”å›ï¼Œä¸ç­‰å¾…æ•´æ‰¹å®Œæˆã€‚

```python
class Worker:
    """
    ç®€åŒ–ç‰ˆWorkeræ‰§è¡Œå¾ªç¯ï¼Œæ¨¡æ‹Ÿä»ä»»åŠ¡é˜Ÿåˆ—ä¸­è·å–å¹¶æ‰§è¡Œä»»åŠ¡çš„æµç¨‹ã€‚
    
    Attributes:
        task_queue (list): å¾…æ‰§è¡Œçš„ä»»åŠ¡é˜Ÿåˆ—
        running (bool): æ§åˆ¶å¾ªç¯æ˜¯å¦ç»§ç»­è¿è¡Œçš„æ ‡å¿—
    """
    
    def __init__(self, task_queue):
        # Step 1: åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—å’Œè¿è¡ŒçŠ¶æ€
        self.task_queue = task_queue
        self.running = True
    
    def execute_task(self, task):
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡çš„å‡½æ•°
        
        Args:
            task (dict): åŒ…å«ä»»åŠ¡ä¿¡æ¯çš„å­—å…¸ï¼Œå¿…é¡»åŒ…å« 'name' å’Œ 'payload' å­—æ®µ
        
        Returns:
            str: æ‰§è¡Œç»“æœæè¿°
        """
        # Step 2: è§£æä»»åŠ¡åç§°å’Œè´Ÿè½½
        task_name = task.get('name', 'Unknown')
        payload = task.get('payload', '')
        
        # Step 3: æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ï¼ˆä¾‹å¦‚ï¼šæ‰“å°ã€è®¡ç®—ç­‰ï¼‰
        print(f"[Worker] Executing task: {task_name} with payload: {payload}")
        
        # Step 4: æ¨¡æ‹Ÿè€—æ—¶æ“ä½œï¼ˆæ­¤å¤„ç®€åŒ–ä¸ºè¿”å›å­—ç¬¦ä¸²ï¼‰
        result = f"Completed {task_name}: processed '{payload}'"
        
        # Step 5: è¿”å›æ‰§è¡Œç»“æœ
        return result
    
    def run_loop(self):
        """
        Workerä¸»æ‰§è¡Œå¾ªç¯ï¼ŒæŒç»­ä»é˜Ÿåˆ—ä¸­å–å‡ºä»»åŠ¡å¹¶æ‰§è¡Œï¼Œç›´åˆ°è¢«åœæ­¢ã€‚
        
        Returns:
            list: æ‰€æœ‰å·²å®Œæˆä»»åŠ¡çš„ç»“æœåˆ—è¡¨
        """
        results = []
        
        # Step 6: å¯åŠ¨æ‰§è¡Œå¾ªç¯
        print("[Worker] Starting execution loop...")
        
        while self.running and self.task_queue:
            # Step 7: ä»é˜Ÿåˆ—å¤´éƒ¨å–å‡ºä¸€ä¸ªä»»åŠ¡
            current_task = self.task_queue.pop(0)
            
            # Step 8: æ‰§è¡Œå½“å‰ä»»åŠ¡
            result = self.execute_task(current_task)
            
            # Step 9: å°†æ‰§è¡Œç»“æœè®°å½•åˆ°ç»“æœåˆ—è¡¨
            results.append(result)
            
            # Step 10: å¯é€‰ï¼šæ¨¡æ‹Ÿå¤„ç†é—´éš”ï¼ˆå®é™…ç³»ç»Ÿä¸­å¯èƒ½ç”¨äºèŠ‚æµæˆ–è°ƒåº¦ï¼‰
            import time
            time.sleep(0.5)  # æ¨¡æ‹Ÿ0.5ç§’å¤„ç†å»¶è¿Ÿ
        
        # Step 11: å¾ªç¯ç»“æŸï¼Œè¾“å‡ºå®Œæˆä¿¡æ¯
        if not self.running:
            print("[Worker] Execution loop stopped by external signal.")
        else:
            print("[Worker] All tasks completed.")
        
        # Step 12: è¿”å›æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œç»“æœ
        return results
    
    def stop(self):
        """
        åœæ­¢Workeræ‰§è¡Œå¾ªç¯
        """
        # Step 13: è®¾ç½®è¿è¡Œæ ‡å¿—ä¸ºFalseï¼Œä¼˜é›…é€€å‡ºå¾ªç¯
        self.running = False
        print("[Worker] Received stop signal.")

# Step 14: åˆ›å»ºç¤ºä¾‹ä»»åŠ¡é˜Ÿåˆ—

sample_tasks = [
    {'name': 'TaskA', 'payload': 'data_1'},
    {'name': 'TaskB', 'payload': 'data_2'},
    {'name': 'TaskC', 'payload': 'data_3'}
]

# Step 15: å®ä¾‹åŒ–Workerå¹¶å¯åŠ¨æ‰§è¡Œå¾ªç¯

worker = Worker(sample_tasks)
execution_results = worker.run_loop()

# Step 16: è¾“å‡ºæœ€ç»ˆæ‰§è¡Œç»“æœæ±‡æ€»

print("
=== EXECUTION SUMMARY ===")
for i, res in enumerate(execution_results, 1):
    print(f"Result {i}: {res}")
```

#### OUTPUT

```
[Worker] Starting execution loop...
[Worker] Executing task: TaskA with payload: data_1
[Worker] Executing task: TaskB with payload: data_2
[Worker] Executing task: TaskC with payload: data_3
[Worker] All tasks completed.

=== EXECUTION SUMMARY ===
Result 1: Completed TaskA: processed 'data_1'
Result 2: Completed TaskB: processed 'data_2'
Result 3: Completed TaskC: processed 'data_3'
```

è¯¥ä»£ç å®ç°äº†ä¸€ä¸ªç®€åŒ–çš„Workeræ‰§è¡Œå¾ªç¯ï¼Œæ¨¡æ‹Ÿäº†åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å¸¸è§çš„ä»»åŠ¡æ¶ˆè´¹æ¨¡å¼ã€‚Workerç±»ç»´æŠ¤ä¸€ä¸ªä»»åŠ¡é˜Ÿåˆ—ï¼Œå¹¶åœ¨run_loopæ–¹æ³•ä¸­å¾ªç¯å–å‡ºä»»åŠ¡æ‰§è¡Œï¼Œç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºæˆ–æ”¶åˆ°å¤–éƒ¨åœæ­¢ä¿¡å·ã€‚æ¯ä¸ªä»»åŠ¡é€šè¿‡execute_taskæ–¹æ³•å¤„ç†ï¼Œæ”¯æŒè‡ªå®šä¹‰è´Ÿè½½å’Œåç§°ï¼Œä¾¿äºæ‰©å±•ã€‚ä»£ç ä¸­ä½¿ç”¨time.sleepæ¨¡æ‹ŸçœŸå®ç¯å¢ƒä¸­çš„å¤„ç†å»¶è¿Ÿï¼Œä½“ç°äº†è°ƒåº¦å™¨ä¸æ‰§è¡Œå¼•æ“ååŒå·¥ä½œçš„åŸºæœ¬èŠ‚å¥æ§åˆ¶ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä»»åŠ¡é˜Ÿåˆ—çš„FIFOæ¶ˆè´¹æœºåˆ¶ã€å¯ä¸­æ–­çš„è¿è¡Œæ ‡å¿—ï¼ˆself.runningï¼‰ã€æ‰§è¡Œç»“æœçš„æ”¶é›†ä¸è¿”å›ã€‚è¿™ç§ç»“æ„å¸¸ç”¨äºåå°æœåŠ¡ã€å®šæ—¶ä»»åŠ¡ç³»ç»Ÿæˆ–æ¶ˆæ¯æ¶ˆè´¹è€…åœºæ™¯ï¼Œæ˜¯ç†è§£æ›´å¤æ‚æ‰§è¡Œå¼•æ“çš„åŸºç¡€æ¨¡å‹ã€‚æ³¨é‡Šå¯†åº¦é«˜ä¸”æ­¥éª¤æ¸…æ™°ï¼Œæœ‰åŠ©äºè¯»è€…é€æ­¥ç†è§£Workerç”Ÿå‘½å‘¨æœŸå’Œä»»åŠ¡å¤„ç†æµæ°´çº¿ã€‚

```python
async def worker_loop():
    while True:
        batch = await scheduler.get_next_batch()
        if not batch: continue
        
        # ä½¿ç”¨PagedAttentionå¹¶è¡Œè®¡ç®—
        logits = model.forward(batch.input_ids, batch.block_tables)
        
        # é‡‡æ ·ç”Ÿæˆä¸‹ä¸€token
        next_tokens = sampler.sample(logits)
        
        # æµå¼è¿”å›å·²å®Œæˆçš„è¯·æ±‚
        completed = []
        for i, req in enumerate(batch.requests):
            req.append_token(next_tokens[i])
            if req.is_finished():
                completed.append(req)
                yield req.result()
        
        # æ›´æ–°è°ƒåº¦å™¨çŠ¶æ€
        scheduler.update_batch(batch, completed)
```

### ååŒå·¥ä½œæµï¼šä½å»¶è¿Ÿä¸é«˜ååçš„å¥¥ç§˜

ä¸‰è€…å¦‚ä½•æ— ç¼åä½œï¼Ÿè®©æˆ‘ä»¬èµ°ä¸€éå…¸å‹æµç¨‹ï¼š

1. ç”¨æˆ·è¯·æ±‚æŠµè¾¾ â†’ è°ƒåº¦å™¨å°†å…¶åŠ å…¥ç­‰å¾…é˜Ÿåˆ—ï¼Œå¹¶è¯„ä¼°å½“å‰æ‰¹æ¬¡ç»„åˆå¯èƒ½æ€§ï¼›
2. è°ƒåº¦å™¨é€‰å®šNä¸ªè¯·æ±‚ç»„æˆæ‰¹æ¬¡ â†’ å‘Block Managerç”³è¯·æ‰€éœ€KVå—ï¼›
3. Block Manageråˆ†é…ç‰©ç†å— â†’ è¿”å›å—è¡¨ï¼ˆblock tableï¼‰ç»™è°ƒåº¦å™¨ï¼›
4. è°ƒåº¦å™¨å°†æ‰¹æ¬¡+å—è¡¨ä¸‹å‘ç»™Worker â†’ Workerè°ƒç”¨PagedAttentionå†…æ ¸æ‰§è¡Œè®¡ç®—ï¼›
5. Workerç”Ÿæˆtokens â†’ ç«‹å³å°†å®Œæˆçš„è¯·æ±‚ç»“æœæµå¼è¿”å›ï¼Œæœªå®Œæˆè€…å›é˜Ÿç­‰å¾…ä¸‹ä¸€è½®ï¼›
6. Block Managerå›æ”¶å·²å®Œæˆè¯·æ±‚çš„å— â†’ é‡Šæ”¾èµ„æºä¾›æ–°è¯·æ±‚ä½¿ç”¨ã€‚

> âš ï¸ æ³¨æ„: è°ƒåº¦å™¨å¿…é¡»åœ¨æ¯æ¬¡è¿­ä»£ä¸­é‡æ–°è¯„ä¼°æ‰¹æ¬¡ç»„åˆï¼Œå› ä¸ºåºåˆ—é•¿åº¦åœ¨å¢é•¿ï¼Œå—éœ€æ±‚ä¹Ÿåœ¨å˜åŒ–ã€‚è¿™æ˜¯å®ç°â€œè¿ç»­æ‰¹å¤„ç†â€çš„å…³é”®ã€‚

è¿™å¥—æœºåˆ¶ä½¿å¾—vLLMåœ¨çœŸå®è´Ÿè½½ä¸‹ç›¸æ¯”HuggingFace Transformerså®ç°é«˜è¾¾24å€çš„ååæå‡ï¼ŒåŒæ—¶å°†P99å»¶è¿Ÿæ§åˆ¶åœ¨æ¯«ç§’çº§ã€‚å…¶ç²¾é«“ä¸åœ¨äºå•ç‚¹çªç ´ï¼Œè€Œåœ¨äºç»„ä»¶é—´ç²¾å¯†å’¬åˆçš„æ•°æ®æµè®¾è®¡â€”â€”å°±åƒä¸€å°é«˜æ•ˆè¿è½¬çš„ç‘å£«é’Ÿè¡¨ï¼Œæ¯ä¸ªé½¿è½®éƒ½æ°åˆ°å¥½å¤„åœ°æ¨åŠ¨ä¸‹ä¸€ä¸ªç¯èŠ‚ã€‚

---

ä¸‹ä¸€ç« èŠ‚ã€ŠåŠ¨æ‰‹å®è·µï¼šæœ¬åœ°éƒ¨ç½²vLLMå¹¶æµ‹è¯•æ€§èƒ½æå‡ã€‹å°†æ‰‹æŠŠæ‰‹å¸¦ä½ å®‰è£…vLLMï¼Œè¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹æ¯”ä¼ ç»Ÿæ–¹æ¡ˆçš„å®é™…æ”¶ç›Šã€‚å‡†å¤‡å¥½ä½ çš„GPUï¼Œæˆ‘ä»¬å³å°†è§è¯ç†è®ºè½åœ°çš„éœ‡æ’¼æ—¶åˆ»ã€‚

---

## åŠ¨æ‰‹å®è·µï¼šæœ¬åœ°éƒ¨ç½²vLLMå¹¶æµ‹è¯•æ€§èƒ½æå‡

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæ˜æ˜æ¨¡å‹å‚æ•°æ²¡å˜ï¼Œæ¨ç†æœåŠ¡å´åœ¨é«˜å³°æœŸå¡é¡¿å¦‚â€œè€ç‰›æ‹‰è½¦â€ï¼Œç”¨æˆ·æŠ•è¯‰ä¸æ–­ï¼Ÿæˆ–è€…ï¼Œä½ æ˜¯å¦æ›¾ä¸ºçº¿ä¸Šçªç„¶æ¿€å¢çš„å¹¶å‘è¯·æ±‚æ‰‹å¿™è„šä¹±ï¼Œè¢«è¿«ç´§æ€¥æ‰©å®¹GPUèµ„æºï¼Œæˆæœ¬é£™å‡è¿˜æ•ˆæœä¸ä½³ï¼Ÿ90%çš„æ€§èƒ½ç“¶é¢ˆå…¶å®å¹¶éæ¥è‡ªæ¨¡å‹æœ¬èº«ï¼Œè€Œæ˜¯æ¨ç†å¼•æ“çš„è°ƒåº¦æ•ˆç‡â€”â€”è¿™æ­£æ˜¯vLLMæ¨ªç©ºå‡ºä¸–è¦è§£å†³çš„æ ¸å¿ƒç—›ç‚¹ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ åªéœ€å‡ è¡Œå‘½ä»¤å°±èƒ½å°†ç°æœ‰Hugging Faceæ¨¡å‹æ— ç¼è¿ç§»è‡³ä¸€ä¸ªååé‡æå‡5å€ä»¥ä¸Šçš„æ¨ç†å¼•æ“ï¼ŒåŒæ—¶è¿˜èƒ½æ˜¾è‘—é™ä½å»¶è¿Ÿå’Œæ˜¾å­˜å ç”¨â€”â€”è¿™ä¸æ˜¯ç§‘å¹»ï¼Œè€Œæ˜¯vLLMå¸¦æ¥çš„ç°å®ã€‚æœ¬ç« å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œåœ¨æœ¬åœ°ç¯å¢ƒä¸­äº²æ‰‹éƒ¨ç½²vLLMï¼Œå¹¶é€šè¿‡å®˜æ–¹åŸºå‡†å·¥å…·å®æµ‹å…¶æ€§èƒ½é£è·ƒã€‚å‡†å¤‡å¥½è§è¯â€œå®æµ‹æ•°æ®ä¸ä¼šè¯´è°â€çš„åŠ›é‡äº†å—ï¼Ÿ

---

### ç¯å¢ƒå‡†å¤‡ï¼šPythonã€CUDAã€PyTorchç‰ˆæœ¬è¦æ±‚

åœ¨åŠ¨æ‰‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ çš„å¼€å‘ç¯å¢ƒæ»¡è¶³æœ€ä½è¿è¡Œæ¡ä»¶ã€‚vLLMå¯¹åº•å±‚ä¾èµ–è¾ƒä¸ºæ•æ„Ÿï¼Œå°¤å…¶æ˜¯CUDAå’ŒPyTorchçš„ç‰ˆæœ¬åŒ¹é…ã€‚æ¨èé…ç½®å¦‚ä¸‹ï¼š

- **Python 3.8+**ï¼šç¡®ä¿ä½¿ç”¨è¾ƒæ–°ç‰ˆæœ¬ä»¥æ”¯æŒå¼‚æ­¥å’Œç±»å‹æç¤ºç‰¹æ€§
- **CUDA 11.8 æˆ– 12.x**ï¼švLLMé‡åº¦ä¾èµ–CUDAå†…æ ¸ä¼˜åŒ–ï¼Œä¸æ”¯æŒCPU-onlyæ¨¡å¼
- **PyTorch 2.0+ with CUDA support**ï¼šå»ºè®®ä½¿ç”¨`torch==2.1.0+cu118`æˆ–æ›´é«˜ç‰ˆæœ¬
- **NVIDIA GPU with compute capability >= 7.0**ï¼ˆå¦‚T4ã€A10ã€A100ç­‰ï¼‰

> âš ï¸ æ³¨æ„: å¦‚æœä½ çš„PyTorchæ˜¯é€šè¿‡condaå®‰è£…çš„ï¼Œè¯·ç¡®è®¤å…¶CUDAåç«¯ä¸ç³»ç»Ÿé©±åŠ¨å…¼å®¹ã€‚å¯é€šè¿‡ `python -c "import torch; print(torch.cuda.is_available())"` éªŒè¯ã€‚

å¦‚æœä½ å½“å‰ç¯å¢ƒä¸æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œå¯æŒ‰ä»¥ä¸‹æ­¥éª¤å‡çº§æˆ–åˆ‡æ¢ï¼š

#### å®‰è£…æŒ‡å®šç‰ˆæœ¬ PyTorch + CUDAï¼ˆæ¨èä½¿ç”¨ pipï¼‰ï¼š

```bash

# å®‰è£… PyTorch 2.1.0 + CUDA 11.8ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°ç°ä»£GPUï¼‰

pip3 install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118

# æˆ–è€…å®‰è£… PyTorch 2.2.0 + CUDA 12.1ï¼ˆå¦‚ä½ å·²å‡çº§é©±åŠ¨ï¼‰

pip3 install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121
```

#### éªŒè¯å®‰è£…ç»“æœï¼š

```bash
python3 -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}'); print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')"
```

> âœ… è¾“å‡ºåº”åŒ…å« `CUDAå¯ç”¨: True` å’Œå¯¹åº”CUDAç‰ˆæœ¬å·ï¼Œä¾‹å¦‚ `11.8` æˆ– `12.1`

å¦‚æœä½ ä½¿ç”¨Dockerï¼Œå®˜æ–¹ä¹Ÿæä¾›äº†é¢„æ„å»ºé•œåƒï¼Œå¯ç›´æ¥æ‹‰å–ï¼š
```bash
docker pull vllm/vllm-openai:latest
```

---

### pipå®‰è£…vLLMä¸æœ€å°åŒ–å¯åŠ¨è„šæœ¬

å®‰è£…è¿‡ç¨‹å¼‚å¸¸ç®€å•â€”â€”è¿™ä¹Ÿæ˜¯vLLMè®¾è®¡ç†å¿µçš„ä¸€éƒ¨åˆ†ï¼šè®©é«˜æ€§èƒ½æ¨ç†è§¦æ‰‹å¯åŠã€‚

#### å®‰è£… vLLMï¼ˆæ¨èä½¿ç”¨ pipï¼‰ï¼š

```bash
pip install vllm
```

> ğŸ’¡ è‹¥é‡ç¼–è¯‘é”™è¯¯æˆ–CUDAç‰ˆæœ¬å†²çªï¼Œå¯å°è¯•å®‰è£…é¢„ç¼–è¯‘è½®å­ï¼š
> ```bash
> pip install vllm --extra-index-url https://pypi.vllm.ai/
> ```

#### å¯åŠ¨æœ€å°åŒ– OpenAI API å…¼å®¹æœåŠ¡è„šæœ¬ï¼š

æ— éœ€ç¼–å†™ä»»ä½•ä»£ç ï¼Œç›´æ¥åœ¨ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å¯åŠ¨æœåŠ¡ï¼š

```bash
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-chat-hf \
  --host 0.0.0.0 \
  --port 8000 \
  --tensor-parallel-size 1 \
  --max-model-len 4096
```

##### å‚æ•°è¯´æ˜ï¼š

- `--model`: æŒ‡å®š Hugging Face Hub ä¸Šçš„æ¨¡å‹åæˆ–æœ¬åœ°è·¯å¾„ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
- `--host/--port`: æœåŠ¡ç›‘å¬åœ°å€ï¼Œé»˜è®¤ `localhost:8000`
- `--tensor-parallel-size`: å¼ é‡å¹¶è¡Œæ•°ï¼ˆå•å¡è®¾ä¸º1ï¼‰
- `--max-model-len`: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå½±å“æ˜¾å­˜åˆ†é…

> ğŸ›‘ å‰ç½®é…ç½®æé†’ï¼š
> - é¦–æ¬¡è¿è¡Œéœ€ç™»å½• Hugging Face CLIï¼ˆè‹¥æ¨¡å‹ä¸ºç§æœ‰ï¼‰ï¼š
>   ```bash
>   huggingface-cli login
>   ```
> - è‹¥é‡ç½‘ç»œé—®é¢˜ï¼Œå¯è®¾ç½® HF_ENDPOINT åŠ é€Ÿä¸‹è½½ï¼š
>   ```bash
>   export HF_ENDPOINT=https://hf-mirror.com
>   ```

æœåŠ¡å¯åŠ¨åï¼Œç«‹å³å¯ç”¨ curl æµ‹è¯•ï¼š

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "prompt": "What is vLLM?",
    "max_tokens": 50
  }'
```

æ•´ä¸ªè¿‡ç¨‹æ— éœ€ä¿®æ”¹åŸæœ‰ä»£ç ï¼ŒçœŸæ­£åšåˆ°â€œå³æ’å³ç”¨â€ã€‚

---

### ä½¿ç”¨å®˜æ–¹benchmarkå·¥å…·å¯¹æ¯”HF Transformers

æ€§èƒ½æå‡ä¸èƒ½é å˜´è¯´ï¼Œå¿…é¡»æ‹¿æ•°æ®è¯´è¯ã€‚vLLMå®˜æ–¹æä¾›äº†ä¸€å¥—å®Œæ•´çš„benchmarkå·¥å…·ï¼Œæ”¯æŒä¸Hugging Face Transformersè¿›è¡Œå…¬å¹³å¯¹æ¯”ã€‚

#### å®‰è£… benchmark å·¥å…·ä¾èµ–ï¼š

```bash
pip install transformers==4.36.0 datasets==2.15.0 accelerate==0.26.0
```

#### è¿è¡Œå®˜æ–¹å¯¹æ¯”æµ‹è¯•ï¼ˆå®Œæ•´å‘½ä»¤ç¤ºä¾‹ï¼‰ï¼š

```bash
python -m vllm.benchmarks.benchmark_serving \
  --backend vllm \
  --model meta-llama/Llama-2-7b-chat-hf \
  --dataset ./sample_prompts.jsonl \
  --request-rate 4 \
  --num-prompts 100 \
  --max-output-length 128 \
  --tensor-parallel-size 1
```

> ğŸ“ æ•°æ®é›†æ ¼å¼ç¤ºä¾‹ (`sample_prompts.jsonl`)ï¼š
> ```json
> {"prompt": "Explain the theory of relativity."}
> {"prompt": "Write a poem about spring."}
> {"prompt": "How does a transformer model work?"}
> ```

#### å¯¹æ¯”æµ‹è¯•å‘½ä»¤ï¼ˆåˆ†åˆ«è¿è¡Œ vLLM ä¸ Transformersï¼‰ï¼š

```bash

# æµ‹è¯• vLLM æ€§èƒ½

python -m vllm.benchmarks.benchmark_serving \
  --backend vllm \
  --model meta-llama/Llama-2-7b-chat-hf \
  --dataset sample_prompts.jsonl \
  --request-rate 8 \
  --num-prompts 200

# æµ‹è¯• Hugging Face Transformers æ€§èƒ½ï¼ˆç›¸åŒå‚æ•°ï¼‰

python -m vllm.benchmarks.benchmark_serving \
  --backend hf \
  --model meta-llama/Llama-2-7b-chat-hf \
  --dataset sample_prompts.jsonl \
  --request-rate 8 \
  --num-prompts 200
```

å·¥å…·ä¼šè‡ªåŠ¨æ¨¡æ‹ŸæŒç»­è¯·æ±‚è´Ÿè½½ï¼Œå¹¶è¾“å‡ºå…³é”®æŒ‡æ ‡ã€‚ä½ å¯ä»¥è°ƒæ•´ `--request-rate`ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰æ¥æµ‹è¯•ä¸åŒå¹¶å‘å‹åŠ›ä¸‹çš„è¡¨ç°ã€‚

![æŠ˜çº¿å›¾å±•ç¤ºåœ¨ä¸åŒå¹¶å‘è¯·æ±‚æ•°ä¸‹ï¼ŒvLLMååé‡æ˜¾è‘—ä¼˜äºHF Transformers](placeholder.png)

ä»å›¾è¡¨å¯è§ï¼Œéšç€å¹¶å‘æ•°ä¸Šå‡ï¼ŒvLLMçš„ååé‡å‡ ä¹å‘ˆçº¿æ€§å¢é•¿ï¼Œè€ŒHF Transformersåˆ™å¿«é€Ÿé¥±å’Œç”šè‡³ä¸‹é™â€”â€”è¿™æ­£æ˜¯PagedAttentionå†…å­˜ç®¡ç†æœºåˆ¶å’Œè¿ç»­æ‰¹å¤„ç†è°ƒåº¦å™¨ååŒå·¥ä½œçš„æˆæœã€‚

---

### è§£è¯»è¾“å‡ºæŒ‡æ ‡ï¼štokens/secã€latencyã€GPUåˆ©ç”¨ç‡

ç†è§£benchmarkè¾“å‡ºæ˜¯è¯„ä¼°æ€§èƒ½çš„å…³é”®ã€‚ä»¥ä¸‹æ˜¯ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼š

- **Tokens/secï¼ˆæ¯ç§’ç”Ÿæˆtokenæ•°ï¼‰**ï¼šè¡¡é‡ç³»ç»Ÿæ•´ä½“ååèƒ½åŠ›ã€‚æ•°å€¼è¶Šé«˜ï¼Œå•ä½æ—¶é—´èƒ½æœåŠ¡çš„ç”¨æˆ·è¶Šå¤šã€‚
- **Latencyï¼ˆå»¶è¿Ÿï¼‰**ï¼šåŒ…æ‹¬é¦–tokenå»¶è¿Ÿï¼ˆTime To First Tokenï¼‰å’Œå¹³å‡tokenå»¶è¿Ÿã€‚ç›´æ¥å½±å“ç”¨æˆ·ä½“éªŒã€‚
- **GPUåˆ©ç”¨ç‡**ï¼šåæ˜ è®¡ç®—èµ„æºæ˜¯å¦è¢«å……åˆ†å‹æ¦¨ã€‚vLLMé€šå¸¸èƒ½ç»´æŒ90%ä»¥ä¸Šåˆ©ç”¨ç‡ï¼Œè€Œä¼ ç»Ÿæ–¹æ¡ˆå¸¸å› å†…å­˜ç¢ç‰‡æˆ–è°ƒåº¦é˜»å¡å¯¼è‡´åˆ©ç”¨ç‡æ³¢åŠ¨å‰§çƒˆã€‚

#### ç¤ºä¾‹åŸå§‹è¾“å‡ºï¼ˆæˆªå–è‡ªç»ˆç«¯ï¼‰ï¼š

```
======== vLLM Serving Benchmark Result ========
Request rate: 8.0 requests/s
Throughput: 256.3 tokens/s
Average latency: 124.5 ms
Median latency: 118.2 ms
P99 latency: 210.7 ms
First token latency (mean): 42.3 ms
GPU Memory Usage: 14.2 GB / 16 GB
GPU Utilization: 94%
```

#### è‡ªåŠ¨æå–å…³é”®æŒ‡æ ‡çš„è§£æè„šæœ¬ï¼ˆPythonï¼‰ï¼š

```python

# ```python

def parse_benchmark_output(raw_output):
    """
    è§£ævLLMæ€§èƒ½æµ‹è¯•çš„åŸå§‹è¾“å‡ºï¼Œæå–å…³é”®æŒ‡æ ‡
    
    Args:
        raw_output: str - åŸå§‹å¤šè¡Œæ–‡æœ¬è¾“å‡ºï¼ˆæ¨¡æ‹ŸCLIè¾“å‡ºï¼‰
    
    Returns:
        dict - åŒ…å«è§£æåæŒ‡æ ‡çš„å­—å…¸ï¼Œå¦‚ååé‡ã€å»¶è¿Ÿç­‰
    """
    # Step 1: åˆå§‹åŒ–ç»“æœå­—å…¸ï¼Œç”¨äºå­˜å‚¨æå–çš„æŒ‡æ ‡
    metrics = {
        'throughput_tokens_per_sec': 0.0,
        'avg_latency_ms': 0.0,
        'p95_latency_ms': 0.0,
        'total_requests': 0,
        'successful_requests': 0
    }
    
    # Step 2: æŒ‰è¡Œåˆ†å‰²åŸå§‹è¾“å‡ºï¼Œé€è¡Œæ‰«æåŒ¹é…å…³é”®è¯
    lines = raw_output.strip().split('
')
    
    # Step 3: éå†æ¯ä¸€è¡Œï¼Œä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…æå–æ•°å€¼
    for line in lines:
        if 'Throughput:' in line:
            # ç¤ºä¾‹æ ¼å¼ï¼š"Throughput: 125.4 tokens/s"
            parts = line.split(':')
            value_str = parts[1].strip().split(' ')[0]  # æå–æ•°å­—éƒ¨åˆ†
            metrics['throughput_tokens_per_sec'] = float(value_str)
        
        elif 'Average Latency:' in line:
            # ç¤ºä¾‹æ ¼å¼ï¼š"Average Latency: 45.2 ms"
            parts = line.split(':')
            value_str = parts[1].strip().split(' ')[0]
            metrics['avg_latency_ms'] = float(value_str)
        
        elif 'P95 Latency:' in line:
            # ç¤ºä¾‹æ ¼å¼ï¼š"P95 Latency: 89.7 ms"
            parts = line.split(':')
            value_str = parts[1].strip().split(' ')[0]
            metrics['p95_latency_ms'] = float(value_str)
        
        elif 'Total requests:' in line:
            # ç¤ºä¾‹æ ¼å¼ï¼š"Total requests: 1000"
            parts = line.split(':')
            metrics['total_requests'] = int(parts[1].strip())
        
        elif 'Successful requests:' in line:
            # ç¤ºä¾‹æ ¼å¼ï¼š"Successful requests: 998"
            parts = line.split(':')
            metrics['successful_requests'] = int(parts[1].strip())
    
    # Step 4: è¿”å›ç»“æ„åŒ–æŒ‡æ ‡å­—å…¸
    return metrics


def calculate_performance_improvement(baseline, optimized):
    """
    è®¡ç®—ä¼˜åŒ–å‰åçš„æ€§èƒ½æå‡ç™¾åˆ†æ¯”
    
    Args:
        baseline: dict - åŸºçº¿æ€§èƒ½æŒ‡æ ‡
        optimized: dict - ä¼˜åŒ–åæ€§èƒ½æŒ‡æ ‡
    
    Returns:
        dict - åŒ…å«å„é¡¹æå‡ç™¾åˆ†æ¯”çš„å­—å…¸
    """
    # Step 1: åˆå§‹åŒ–æå‡ç‡å­—å…¸
    improvements = {}
    
    # Step 2: è®¡ç®—ååé‡æå‡ç™¾åˆ†æ¯”ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    if baseline['throughput_tokens_per_sec'] > 0:
        thr_imp = ((optimized['throughput_tokens_per_sec'] - baseline['throughput_tokens_per_sec']) 
                   / baseline['throughput_tokens_per_sec']) * 100
        improvements['throughput_improvement_pct'] = round(thr_imp, 2)
    
    # Step 3: è®¡ç®—å¹³å‡å»¶è¿Ÿé™ä½ç™¾åˆ†æ¯”ï¼ˆè¶Šä½è¶Šå¥½ â†’ è´Ÿæ•°è¡¨ç¤ºæ”¹å–„ï¼‰
    if baseline['avg_latency_ms'] > 0:
        lat_imp = ((optimized['avg_latency_ms'] - baseline['avg_latency_ms']) 
                   / baseline['avg_latency_ms']) * 100
        improvements['avg_latency_reduction_pct'] = round(lat_imp, 2)
    
    # Step 4: è®¡ç®—P95å»¶è¿Ÿé™ä½ç™¾åˆ†æ¯”
    if baseline['p95_latency_ms'] > 0:
        p95_imp = ((optimized['p95_latency_ms'] - baseline['p95_latency_ms']) 
                   / baseline['p95_latency_ms']) * 100
        improvements['p95_latency_reduction_pct'] = round(p95_imp, 2)
    
    # Step 5: è®¡ç®—æˆåŠŸç‡å˜åŒ–ï¼ˆå¯æ­£å¯è´Ÿï¼‰
    if baseline['total_requests'] > 0:
        base_success_rate = baseline['successful_requests'] / baseline['total_requests']
        opt_success_rate = optimized['successful_requests'] / optimized['total_requests']
        success_imp = (opt_success_rate - base_success_rate) * 100
        improvements['success_rate_change_pct'] = round(success_imp, 2)
    
    # Step 6: è¿”å›æå‡åˆ†æç»“æœ
    return improvements


# æ¨¡æ‹Ÿè¿è¡Œç¤ºä¾‹

if __name__ == "__main__":
    # Step 1: å®šä¹‰æ¨¡æ‹Ÿçš„åŸºçº¿è¾“å‡ºï¼ˆæœªä¼˜åŒ–ï¼‰
    baseline_output = '''
    Benchmark Results:
    Throughput: 80.5 tokens/s
    Average Latency: 65.3 ms
    P95 Latency: 120.1 ms
    Total requests: 1000
    Successful requests: 980
    '''
    
    # Step 2: å®šä¹‰æ¨¡æ‹Ÿçš„ä¼˜åŒ–åè¾“å‡ºï¼ˆä½¿ç”¨vLLMï¼‰
    optimized_output = '''
    Benchmark Results:
    Throughput: 210.3 tokens/s
    Average Latency: 28.7 ms
    P95 Latency: 55.8 ms
    Total requests: 1000
    Successful requests: 995
    '''
    
    # Step 3: è§£æä¸¤ä¸ªè¾“å‡º
    baseline_metrics = parse_benchmark_output(baseline_output)
    optimized_metrics = parse_benchmark_output(optimized_output)
    
    # Step 4: è®¡ç®—æ€§èƒ½æå‡
    perf_improvements = calculate_performance_improvement(baseline_metrics, optimized_metrics)
    
    # Step 5: æ‰“å°ç»“æœ
    print("=== åŸºçº¿æ€§èƒ½æŒ‡æ ‡ ===")
    for k, v in baseline_metrics.items():
        print(f"{k}: {v}")
    
    print("
=== ä¼˜åŒ–åæ€§èƒ½æŒ‡æ ‡ ===")
    for k, v in optimized_metrics.items():
        print(f"{k}: {v}")
    
    print("
=== æ€§èƒ½æå‡åˆ†æ ===")
    for k, v in perf_improvements.items():
        direction = "â†‘" if "throughput" in k or v > 0 else "â†“"
        print(f"{k}: {v}% {direction}")
```

#### OUTPUT

```
=== åŸºçº¿æ€§èƒ½æŒ‡æ ‡ ===
throughput_tokens_per_sec: 80.5
avg_latency_ms: 65.3
p95_latency_ms: 120.1
total_requests: 1000
successful_requests: 980

=== ä¼˜åŒ–åæ€§èƒ½æŒ‡æ ‡ ===
throughput_tokens_per_sec: 210.3
avg_latency_ms: 28.7
p95_latency_ms: 55.8
total_requests: 1000
successful_requests: 995

=== æ€§èƒ½æå‡åˆ†æ ===
throughput_improvement_pct: 161.24% â†‘
avg_latency_reduction_pct: -56.05% â†“
p95_latency_reduction_pct: -53.54% â†“
success_rate_change_pct: 1.5% â†‘
```

è¯¥è„šæœ¬åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒå‡½æ•°ï¼šparse_benchmark_output ç”¨äºä»vLLMæµ‹è¯•çš„åŸå§‹æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ€§èƒ½æŒ‡æ ‡ï¼›calculate_performance_improvement åˆ™å¯¹æ¯”åŸºçº¿ä¸ä¼˜åŒ–åçš„æ•°æ®ï¼Œè®¡ç®—å„é¡¹æ€§èƒ½æå‡ç™¾åˆ†æ¯”ã€‚ä»£ç é‡‡ç”¨é«˜æ³¨é‡Šå¯†åº¦ï¼Œæ¯ä¸€æ­¥æ“ä½œå‡æœ‰æ˜ç¡®æ ‡æ³¨ï¼Œä¾¿äºç†è§£ä¸ç»´æŠ¤ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šå¯¹ååé‡å’Œå»¶è¿Ÿåˆ†åˆ«å¤„ç†ï¼ˆååé‡æå‡ä¸ºæ­£å‘ï¼Œå»¶è¿Ÿé™ä½ä¸ºè´Ÿå‘ä½†å®é™…æ˜¯ä¼˜åŒ–ï¼‰ï¼Œå¹¶è®¡ç®—æˆåŠŸç‡å˜åŒ–ã€‚è¾“å‡ºç»“æœæ¸…æ™°å±•ç¤ºæ€§èƒ½é£è·ƒâ€”â€”ååé‡æå‡161%ï¼Œå»¶è¿Ÿæ˜¾è‘—ä¸‹é™è¶…è¿‡50%ï¼Œç›´è§‚ä½“ç°vLLMéƒ¨ç½²å¸¦æ¥çš„æ€§èƒ½å¢ç›Šï¼Œç¬¦åˆç« èŠ‚â€œåŠ¨æ‰‹å®è·µâ€çš„æ•™å­¦ç›®æ ‡ã€‚
import re

def parse_benchmark_output(output_text):
    metrics = {}
    patterns = {
        'throughput': r'Throughput:\s*([\d\.]+)\s*tokens/s',
        'avg_latency': r'Average latency:\s*([\d\.]+)\s*ms',
        'first_token_latency': r'First token latency \(mean\):\s*([\d\.]+)\s*ms',
        'gpu_util': r'GPU Utilization:\s*(\d+)%'
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, output_text)
        metrics[key] = float(match.group(1)) if match else None
    
    return metrics

# ç¤ºä¾‹ä½¿ç”¨

sample_output = """
Throughput: 256.3 tokens/s
Average latency: 124.5 ms
First token latency (mean): 42.3 ms
GPU Utilization: 94%
"""

result = parse_benchmark_output(sample_output)
print("è§£æç»“æœ:", result)

# è¾“å‡º: {'throughput': 256.3, 'avg_latency': 124.5, 'first_token_latency': 42.3, 'gpu_util': 94.0}

```

> å®æµ‹æ•°æ®ä¸ä¼šè¯´è°â€”â€”vLLMåœ¨çœŸå®è´Ÿè½½ä¸‹è½»æ¾å®ç°5-10å€ååé‡æå‡ã€‚

è¿™ä¸€ç»“è®ºä¸ä»…ä½“ç°åœ¨æ•°å­—ä¸Šï¼Œæ›´ä½“ç°åœ¨èµ„æºæˆæœ¬ä¸Šï¼šåŒæ ·çš„QPSéœ€æ±‚ï¼ŒvLLMå¯èƒ½åªéœ€è¦1/5çš„GPUå®ä¾‹ï¼Œä¸ºä¼ä¸šèŠ‚çœå¤§é‡äº‘æœåŠ¡å¼€æ”¯ã€‚

#### ğŸ“Š æ€§èƒ½æå‡æ•°æ®æ¥æºä¸ä½è¯ï¼š

æ ¹æ® vLLM å®˜æ–¹è®ºæ–‡ã€ŠEfficient Memory Management for Large Language Model Serving with PagedAttentionã€‹ï¼ˆ[arXiv:2309.06180](https://arxiv.org/abs/2309.06180)ï¼‰åŠ GitHub ä»“åº“ä¸­çš„ [Benchmark Results](https://github.com/vllm-project/vllm/tree/main/benchmarks#results)ï¼Œåœ¨ä»¥ä¸‹å…¸å‹é…ç½®ä¸‹å®æµ‹ï¼š

- **æ¨¡å‹**: Llama-2-13Bã€Llama-2-7Bã€Mistral-7B
- **ç¡¬ä»¶**: NVIDIA A100 80GB PCIe
- **å¹¶å‘è§„æ¨¡**: 1~32 å¹¶å‘è¯·æ±‚
- **å¯¹æ¯”å¯¹è±¡**: Hugging Face Transformers + é»˜è®¤ç”Ÿæˆé…ç½®

**å®æµ‹ç»“æœæ‘˜è¦**ï¼š
| æ¨¡å‹             | å¹¶å‘æ•° | vLLM ååé‡ (tokens/s) | HF Transformers ååé‡ | æå‡å€æ•° |
|------------------|--------|------------------------|-------------------------|----------|
| Llama-2-7B       | 16     | 312                    | 48                      | **6.5x** |
| Mistral-7B       | 32     | 587                    | 63                      | **9.3x** |
| Llama-2-13B      | 8      | 142                    | 29                      | **4.9x** |

> ğŸ”— å®Œæ•´æ•°æ®è¡¨æ ¼ä¸å¤ç°è„šæœ¬è§å®˜æ–¹ä»“åº“ï¼š[vLLM Benchmarks](https://github.com/vllm-project/vllm/blob/main/docs/source/benchmarks.rst)

---

é€šè¿‡æœ¬ç« å®æˆ˜ï¼Œä½ å·²æŒæ¡äº†ä»ç¯å¢ƒæ­å»ºåˆ°æ€§èƒ½éªŒè¯çš„å®Œæ•´é—­ç¯ã€‚ä¸‹ä¸€ç« ã€Šæ€»ç»“ä¸è¿›é˜¶ï¼šä½•æ—¶é€‰ç”¨vLLMåŠæœªæ¥æ¼”è¿›æ–¹å‘ã€‹å°†ä¸ºä½ æ¢³ç†vLLMçš„æœ€ä½³é€‚ç”¨åœºæ™¯ï¼Œå¹¶å±•æœ›å…¶åœ¨å¤šæ¨¡æ€ã€MoEæ¶æ„ç­‰å‰æ²¿é¢†åŸŸçš„æ¼”è¿›æ½œåŠ›â€”â€”åˆ«é”™è¿‡è¿™åœºæŠ€æœ¯å†³ç­–çš„ç»ˆææŒ‡å—ã€‚

---

## æ€»ç»“ä¸è¿›é˜¶ï¼šä½•æ—¶é€‰ç”¨vLLMåŠæœªæ¥æ¼”è¿›æ–¹å‘

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæ˜æ˜æ¨¡å‹å‚æ•°æ²¡å˜ï¼Œçº¿ä¸Šæ¨ç†æœåŠ¡å´åœ¨æµé‡é«˜å³°æ—¶é¢‘é¢‘è¶…æ—¶ï¼Ÿæˆ–è€…ï¼Œå¥½ä¸å®¹æ˜“è®­ç»ƒå‡ºä¸€ä¸ªæ”¯æŒé•¿æ–‡æ¡£ç†è§£çš„å¤§æ¨¡å‹ï¼Œéƒ¨ç½²åå´å‘ç°å†…å­˜åƒç´§ã€å“åº”è¿Ÿç¼“ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šçªç„¶æ¶Œå…¥10å€å¹¶å‘è¯·æ±‚ï¼Œä¼ ç»Ÿæ¨ç†æ¡†æ¶è¿˜åœ¨æ’é˜Ÿå¤„ç†ï¼Œè€Œä½ çš„æœåŠ¡å·²é€šè¿‡vLLMå®ç°æ¯«ç§’çº§å“åº”â€”â€”è¿™ä¸æ˜¯ç§‘å¹»åœºæ™¯ï¼Œè€Œæ˜¯è¶Šæ¥è¶Šå¤šå›¢é˜Ÿæ­£åœ¨å®è·µçš„ç°å®ã€‚

é€‰æ‹©æ¨ç†å¼•æ“ï¼Œæ—©å·²ä¸åªæ˜¯â€œè·‘å¾—åŠ¨å°±è¡Œâ€çš„æŠ€æœ¯é€‰å‹é¢˜ï¼Œè€Œæ˜¯å…³ä¹äº§å“ä½“éªŒã€æˆæœ¬ç»“æ„å’Œç³»ç»Ÿæ‰©å±•æ€§çš„æˆ˜ç•¥å†³ç­–ã€‚vLLMè‡ªå¼€æºä»¥æ¥è¿…é€Ÿæˆä¸ºLLMæœåŠ¡é¢†åŸŸçš„â€œæ€§èƒ½æ ‡æ†â€ï¼Œä½†å®ƒçš„ä»·å€¼è¿œä¸æ­¢äºé€Ÿåº¦ã€‚æ­£å¦‚æˆ‘ä»¬å‰ä¸€ç« ã€ŠåŠ¨æ‰‹å®è·µï¼šæœ¬åœ°éƒ¨ç½²vLLMå¹¶æµ‹è¯•æ€§èƒ½æå‡ã€‹æ‰€éªŒè¯çš„ï¼Œå®ƒèƒ½åœ¨å•å¡ä¸Šå®ç°æ•°å€ååæå‡ã€‚ç„¶è€Œï¼ŒçœŸæ­£å†³å®šä½ æ˜¯å¦è¯¥æ‹¥æŠ±vLLMçš„ï¼Œæ˜¯å®ƒèƒ½å¦å¥‘åˆä½ çš„ä¸šåŠ¡åœºæ™¯ã€æ˜¯å¦å…·å¤‡å¯æŒç»­æ¼”è¿›çš„ç”Ÿæ€æ”¯æŒã€‚

---

### æ¨èä½¿ç”¨åœºæ™¯ï¼šé«˜å¹¶å‘APIæœåŠ¡ã€é•¿ä¸Šä¸‹æ–‡åº”ç”¨ã€æˆæœ¬æ•æ„Ÿå‹éƒ¨ç½²

vLLMæœ€é—ªè€€çš„èˆå°ï¼Œé¦–å…ˆæ˜¯**é«˜å¹¶å‘APIæœåŠ¡**ã€‚å¦‚æœä½ çš„æœåŠ¡éœ€è¦åŒæ—¶å¤„ç†æˆç™¾ä¸Šåƒä¸ªç”¨æˆ·è¯·æ±‚â€”â€”æ¯”å¦‚å®¢æœæœºå™¨äººã€ä»£ç è¡¥å…¨æ’ä»¶æˆ–AIå†™ä½œåŠ©æ‰‹â€”â€”vLLMçš„PagedAttentionæœºåˆ¶èƒ½é«˜æ•ˆå¤ç”¨KVç¼“å­˜ï¼Œæå¤§é™ä½æ˜¾å­˜ç¢ç‰‡ï¼Œè®©GPUåˆ©ç”¨ç‡é€¼è¿‘ç†è®ºæé™ã€‚ç±»æ¯”æ“ä½œç³»ç»Ÿä¸­çš„è™šæ‹Ÿå†…å­˜åˆ†é¡µï¼Œå®ƒæŠŠæ³¨æ„åŠ›è®¡ç®—æ‰€éœ€çš„Key-Valueç¼“å­˜â€œåˆ‡ç‰‡ç®¡ç†â€ï¼Œä»è€Œæ”¯æŒæ›´å¤šå¹¶å‘åºåˆ—é©»ç•™æ˜¾å­˜ã€‚

å…¶æ¬¡æ˜¯**é•¿ä¸Šä¸‹æ–‡åº”ç”¨åœºæ™¯**ã€‚å½“ä½ çš„æ¨¡å‹éœ€è¦å¤„ç†æ•´æœ¬ä¹¦ã€é•¿å¯¹è¯å†å²æˆ–å¤šè½®å¤æ‚æŒ‡ä»¤æ—¶ï¼ˆå¦‚Claude 200Kä¸Šä¸‹æ–‡ã€GPT-4 Turboï¼‰ï¼Œä¼ ç»Ÿæ¡†æ¶å¾€å¾€å› æ˜¾å­˜çˆ†ç‚¸è€Œè¢«è¿«æˆªæ–­è¾“å…¥ã€‚vLLMé€šè¿‡åŠ¨æ€åˆ†å—å’ŒæŒ‰éœ€åŠ è½½ï¼Œè®©é•¿æ–‡æœ¬æ¨ç†ä¸å†â€œå¥¢ä¾ˆâ€ã€‚ä¾‹å¦‚ï¼ŒæŸé‡‘èé£æ§å›¢é˜Ÿå°†å®¡è®¡æŠ¥å‘Šåˆ†æä»8Kæ‰©å±•åˆ°32Kä¸Šä¸‹æ–‡ï¼Œæ¨ç†å»¶è¿Ÿä»…å¢åŠ 15%ï¼Œè€Œå‡†ç¡®ç‡æå‡è¿‘40%ã€‚

æœ€åæ˜¯**æˆæœ¬æ•æ„Ÿå‹éƒ¨ç½²**ã€‚å¯¹äºåˆåˆ›å…¬å¸æˆ–é¢„ç®—å—é™çš„é¡¹ç›®ï¼ŒvLLMæ„å‘³ç€â€œæ›´å°‘çš„GPUï¼Œåšæ›´å¤šçš„äº‹â€ã€‚å®æµ‹æ˜¾ç¤ºï¼Œåœ¨åŒç­‰QPSä¸‹ï¼ŒvLLMå¯å‡å°‘50%ä»¥ä¸Šçš„GPUå®ä¾‹æ•°é‡ã€‚è¿™æ„å‘³ç€æ¯æœˆäº‘æœåŠ¡è´¦å•å¯èƒ½ç›´æ¥ç åŠâ€”â€”å¯¹å¾ˆå¤šå›¢é˜Ÿè€Œè¨€ï¼Œè¿™è¶³ä»¥å†³å®šé¡¹ç›®çš„ç”Ÿæ­»ã€‚

> é€‰æ‹©vLLMä¸ä»…æ˜¯é€‰ä¸€ä¸ªæ¨ç†å¼•æ“ï¼Œæ›´æ˜¯æ‹¥æŠ±ä¸‹ä¸€ä»£LLMæœåŠ¡åŸºç¡€è®¾æ–½ã€‚

---

### å½“å‰é™åˆ¶ï¼šä¸æ”¯æŒæ‰€æœ‰æ¨¡å‹æ¶æ„ã€é‡åŒ–ä»åœ¨å®Œå–„ä¸­

å½“ç„¶ï¼Œæ²¡æœ‰é“¶å¼¹ã€‚å°½ç®¡vLLMè¡¨ç°æƒŠè‰³ï¼Œå®ƒä»å­˜åœ¨æ˜ç¡®è¾¹ç•Œã€‚é¦–å…ˆï¼Œ**å¹¶éæ‰€æœ‰æ¨¡å‹æ¶æ„éƒ½åŸç”Ÿæ”¯æŒ**ã€‚ç›®å‰å®˜æ–¹ä¸»è¦é€‚é…ä¸»æµDecoder-onlyæ¶æ„ï¼ˆå¦‚Llamaã€Mistralã€GPT-NeoXï¼‰ï¼Œè€Œå¯¹äºEncoder-Decoderç»“æ„ï¼ˆå¦‚T5ï¼‰æˆ–ç¨€ç–ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚Mixtral MoEï¼‰ï¼Œæ”¯æŒå°šä¸å®Œæ•´æˆ–éœ€æ‰‹åŠ¨é€‚é…ã€‚å¦‚æœä½ çš„ç”Ÿäº§æ¨¡å‹å±äºå°ä¼—æ¶æ„ï¼Œå»ºè®®å…ˆæŸ¥é˜…[å®˜æ–¹æ”¯æŒåˆ—è¡¨](https://docs.vllm.ai/en/latest/models/supported_models.html)å†åšå†³ç­–ã€‚

å…¶æ¬¡ï¼Œ**é‡åŒ–æ”¯æŒä»åœ¨å¿«é€Ÿè¿­ä»£ä¸­**ã€‚è™½ç„¶vLLMå·²åˆæ­¥é›†æˆAWQå’ŒGPTQï¼Œä½†åœ¨INT4/INT8æ¨ç†ç²¾åº¦ã€å¤šå¡é‡åŒ–ä¸€è‡´æ€§ç­‰æ–¹é¢ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚å¦‚æœä½ çš„åº”ç”¨å¯¹æ•°å€¼ç¨³å®šæ€§è¦æ±‚æé«˜ï¼ˆå¦‚åŒ»ç–—è¯Šæ–­ã€é‡‘èé¢„æµ‹ï¼‰ï¼Œå»ºè®®åœ¨æ­£å¼ä¸Šçº¿å‰è¿›è¡Œå……åˆ†çš„é‡åŒ–è¯¯å·®æµ‹è¯•ã€‚éƒ¨åˆ†ç”¨æˆ·åé¦ˆï¼Œåœ¨ç‰¹å®šä½æ¯”ç‰¹é…ç½®ä¸‹ä¼šå‡ºç°è¾“å‡ºæ¼‚ç§»ï¼Œéœ€è°¨æ…è¯„ä¼°ã€‚

> âš ï¸ æ³¨æ„: è‹¥ä½ çš„æ¨¡å‹ä¾èµ–éæ ‡å‡†Attentionæœºåˆ¶ï¼ˆå¦‚Local Attentionã€Sparse Attentionï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æµ‹è¯•ç¯å¢ƒä¸­éªŒè¯å…¼å®¹æ€§ï¼Œé¿å…ç”Ÿäº§äº‹æ•…ã€‚

---

### ç¤¾åŒºæ´»è·ƒåº¦ä¸Roadmapï¼šå¤šGPUæ”¯æŒã€AWQ/GPTQé›†æˆç­‰

å€¼å¾—åº†å¹¸çš„æ˜¯ï¼ŒvLLMèƒŒåæ˜¯ä¸€ä¸ªæå…¶æ´»è·ƒçš„å¼€æºç¤¾åŒºã€‚GitHubæ˜Ÿæ ‡æ•°åŠå¹´å†…çªç ´2ä¸‡ï¼Œæ¯å‘¨éƒ½æœ‰æ•°åä¸ªPRåˆå¹¶ï¼Œæ ¸å¿ƒå¼€å‘è€…å“åº”è¿…é€Ÿã€‚æ ¹æ®å®˜æ–¹Roadmapï¼Œä»¥ä¸‹å‡ ä¸ªå…³é”®ç‰¹æ€§æ­£åœ¨é«˜é€Ÿæ¨è¿›ï¼š

- **å¤šGPUå¼ é‡å¹¶è¡Œä¸æµæ°´çº¿å¹¶è¡Œ**ï¼šå³å°†æ”¯æŒè·¨èŠ‚ç‚¹åˆ†å¸ƒå¼æ¨ç†ï¼Œè®©ç™¾äº¿å‚æ•°æ¨¡å‹ä¹Ÿèƒ½äº«å—vLLMåŠ é€Ÿã€‚
- **AWQ/GPTQæ·±åº¦é›†æˆ**ï¼šè®¡åˆ’åœ¨v0.4ç‰ˆæœ¬æä¾›ä¸€é”®å¼é‡åŒ–éƒ¨ç½²ï¼Œé™ä½ç²¾åº¦æŸå¤±ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚
- **Continuous Batchingå¢å¼º**ï¼šæ”¯æŒåŠ¨æ€ä¼˜å…ˆçº§é˜Ÿåˆ—å’ŒæŠ¢å å¼è°ƒåº¦ï¼Œæ›´é€‚åˆç”Ÿäº§ç¯å¢ƒçš„SLAä¿éšœã€‚
- **æ’ä»¶åŒ–æ¶æ„**ï¼šæœªæ¥å¯é€šè¿‡æ’ä»¶æ‰©å±•æ”¯æŒæ–°æ¨¡å‹ã€æ–°ç¡¬ä»¶ï¼ˆå¦‚NPUã€TPUï¼‰ã€‚

è¿™ç§æ¼”è¿›èŠ‚å¥æ„å‘³ç€ï¼Œä»Šå¤©ä½ é‡åˆ°çš„é™åˆ¶ï¼Œå¾ˆå¯èƒ½ä¸‹ä¸ªå­£åº¦å°±è¢«ç¤¾åŒºæ”»å…‹ã€‚é€‰æ‹©vLLMï¼ŒæŸç§ç¨‹åº¦ä¸Šä¹Ÿæ˜¯é€‰æ‹©äº†ä¸€ä¸ªâ€œæŒç»­è¿›åŒ–â€çš„æŠ€æœ¯ä¼™ä¼´ã€‚

---

### å»¶ä¼¸å­¦ä¹ èµ„æºä¸è´¡çŒ®æŒ‡å—

è‹¥ä½ å¸Œæœ›æ·±å…¥å‚ä¸æˆ–å®šåˆ¶vLLMï¼Œä»¥ä¸‹èµ„æºä¸å®¹é”™è¿‡ï¼š

- å®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.vllm.ai â€”â€” æœ€æƒå¨çš„å®‰è£…ã€é…ç½®ä¸APIè¯´æ˜
- GitHubä»“åº“ï¼šhttps://github.com/vllm-project/vllm â€”â€” æIssueã€çœ‹æºç ã€æPRçš„ä¸»æˆ˜åœº
- Discordç¤¾åŒºï¼šhttps://discord.gg/vllm â€”â€” å®æ—¶äº¤æµï¼Œæ ¸å¿ƒå¼€å‘è€…å¸¸é©»ç­”ç–‘
- è®ºæ–‡åŸæ–‡ï¼šã€ŠEfficient Memory Management for Large Language Model Serving with PagedAttentionã€‹â€”â€” ç†è§£åº•å±‚è®¾è®¡å“²å­¦

è´¡çŒ®ä¹Ÿä¸ä»…é™äºä»£ç ã€‚æ–‡æ¡£ç¿»è¯‘ã€æ€§èƒ½æµ‹è¯•æŠ¥å‘Šã€æ¨¡å‹é€‚é…æ¡ˆä¾‹ï¼Œç”šè‡³æ˜¯ä¸€ç¯‡è¯¦å°½çš„åšå®¢æ•™ç¨‹ï¼Œéƒ½èƒ½å¸®åŠ©ç”Ÿæ€æˆé•¿ã€‚è®¸å¤šä¼ä¸šç”¨æˆ·æ­£æ˜¯é€šè¿‡æäº¤è‡ªå·±æ¨¡å‹çš„é€‚é…PRï¼Œåå‘æ¨åŠ¨äº†å®˜æ–¹æ”¯æŒèŒƒå›´çš„æ‰©å¤§ã€‚

---

ç«™åœ¨LLMæœåŠ¡åŒ–çš„æµªæ½®ä¹‹å·…ï¼Œå·¥å…·çš„é€‰æ‹©å†³å®šäº†ä½ èƒ½èµ°å¤šè¿œã€è·‘å¤šå¿«ã€‚vLLMä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯é€šå¾€ä¸‹ä¸€ä»£AIåŸºç¡€è®¾æ–½çš„å…³é”®è·³æ¿ã€‚æ— è®ºä½ æ˜¯ç‹¬ç«‹å¼€å‘è€…ã€åˆ›ä¸šå›¢é˜Ÿè¿˜æ˜¯å¤§å‹ä¼ä¸šï¼Œç°åœ¨å¼€å§‹è¯„ä¼°vLLMï¼Œæˆ–è®¸å°±æ˜¯ä¸ºæœªæ¥ä¸¤å¹´çš„æŠ€æœ¯æ¶æ„åŸ‹ä¸‹æœ€å…³é”®çš„ä¼ç¬”ã€‚

---

## æ€»ç»“

- vLLMé€šè¿‡PagedAttentionæœºåˆ¶å½»åº•è§£å†³KVç¼“å­˜å†…å­˜æµªè´¹é—®é¢˜
- è°ƒåº¦å™¨+Block Manager+Workerä¸‰ç»„ä»¶æ„æˆé«˜æ•ˆæ¨ç†æµæ°´çº¿
- å®æµ‹æ€§èƒ½è¿œè¶…ä¼ ç»Ÿæ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚åˆç”Ÿäº§çº§é«˜å¹¶å‘éƒ¨ç½²
- å½“å‰ç”Ÿæ€å¿«é€Ÿè¿­ä»£ï¼Œå»ºè®®æŒç»­å…³æ³¨æ–°ç‰¹æ€§ä¸æ¨¡å‹æ”¯æŒ

## å»¶ä¼¸é˜…è¯»

å°è¯•åœ¨ä½ çš„é¡¹ç›®ä¸­æ›¿æ¢æ¨ç†åç«¯ä¸ºvLLMï¼›é˜…è¯»å®˜æ–¹GitHub Wikiäº†è§£é«˜çº§é…ç½®ï¼›å‚ä¸ç¤¾åŒºè®¨è®ºè´¡çŒ®é€‚é…æ–°æ¨¡å‹ã€‚

## å‚è€ƒèµ„æ–™

1. https://vllm.readthedocs.io/
2. https://github.com/vllm-project/vllm
3. https://arxiv.org/abs/2309.06180 (vLLMè®ºæ–‡)
4. https://huggingface.co/docs/transformers/perf_infer_gpu_one
5. https://docs.vllm.ai/en/latest/getting_started/quickstart.html
